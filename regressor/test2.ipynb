{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(num_samples=1000, signal_length=100, seed=None):\n",
    "    generator = DataGenerator(signal_length=signal_length, seed=seed)\n",
    "\n",
    "    # Generate full dataset\n",
    "    X_jump, (y_amp_jump, y_cp_jump) = generator.generate_data(num_samples // 2, jump=True)\n",
    "    y_jump_indicator = torch.ones_like(y_cp_jump)\n",
    "    X_const, (y_amp_const, y_cp_const) = generator.generate_data(num_samples // 2, jump=False)\n",
    "    y_const_indicator = torch.zeros_like(y_cp_const)\n",
    "\n",
    "    # Convert everything to float32 explicitly\n",
    "    X = torch.cat([X_jump, X_const], dim=0).float()\n",
    "    y_amp = torch.cat([y_amp_jump, y_amp_const], dim=0).float()\n",
    "    y_cp = torch.cat([y_cp_jump, y_cp_const], dim=0).float().unsqueeze(1)\n",
    "    y_indicator = torch.cat([y_jump_indicator, y_const_indicator], dim=0).float().unsqueeze(1)\n",
    "    print(y_amp.shape, y_cp.shape, y_indicator.shape)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    indices = torch.randperm(num_samples)\n",
    "    X, y_amp, y_cp, y_indicator = X[indices], y_amp[indices], y_cp[indices], y_indicator[indices]\n",
    "\n",
    "    return X, y_indicator, y_cp, y_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ground_truth(jump_positions, n_bins=64, signal_length=1024):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        jump_positions: List of jump positions (in samples) for each signal. Use -1 for no jump.\n",
    "        n_bins: Number of bins for position prediction.\n",
    "        signal_length: Length of the input signal.\n",
    "    Returns:\n",
    "        gt_pos: Tensor of shape [batch_size] (bin index of jump, or 0 if no jump).\n",
    "    \"\"\"\n",
    "    batch_size = len(jump_positions)\n",
    "    gt_pos = torch.zeros(batch_size, dtype=torch.long)\n",
    "    \n",
    "    bin_size = signal_length // n_bins\n",
    "    \n",
    "    for i, pos in enumerate(jump_positions):\n",
    "        if pos >= 0:  # Jump exists\n",
    "            gt_pos[i] = min(pos // bin_size, n_bins - 1)  # Ensure bin index is within range\n",
    "        else:  # No jump\n",
    "            gt_pos[i] = 0  # Irrelevant, since loss will be masked\n",
    "    \n",
    "    return gt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(num_samples=1000, signal_length=1024, batch_size=32, train_ratio=0.8, n_bins=64, seed=None):\n",
    "    \n",
    "    X, y_indicator, y_cp, y_amp = prepare_data(num_samples, signal_length, seed)\n",
    "\n",
    "    y_cp = prepare_ground_truth(y_cp, n_bins, signal_length)\n",
    "\n",
    "    num_samples = X.shape[0]\n",
    "    # Train/validation split\n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = num_samples - train_size\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    X_val = X[train_size:]\n",
    "    y_indicator_train = y_indicator[:train_size]\n",
    "    y_indicator_val = y_indicator[train_size:]\n",
    "    y_cp_train = y_cp[:train_size]\n",
    "    y_cp_val = y_cp[train_size:]\n",
    "    y_amp_train = y_amp[:train_size]\n",
    "    y_amp_val = y_amp[train_size:]\n",
    "\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train, y_indicator_train, y_cp_train, y_amp_train)\n",
    "    val_dataset = TensorDataset(X_val, y_indicator_val, y_cp_val, y_amp_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 4]) torch.Size([50000, 1]) torch.Size([50000, 1])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = prepare_dataloaders(batch_size=256, signal_length=1024, seed=42, num_samples=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[  128.9844, -2362.7139, -1459.9290,  ..., -2437.4631,\n",
       "            -418.7927, -1157.3844],\n",
       "          [ 1056.9888,  -727.7728,  2172.7349,  ...,    67.7678,\n",
       "             858.4960,  2347.1743]],\n",
       " \n",
       "         [[-2390.9436, -1202.0691, -1325.5686,  ..., -1055.2358,\n",
       "            -599.0897, -1452.9836],\n",
       "          [  618.1628,  2084.3911,  2036.7823,  ...,   720.4336,\n",
       "            1206.6832, -1961.8562]],\n",
       " \n",
       "         [[-3103.1084,    10.3209,   225.3537,  ...,  1528.3220,\n",
       "            -596.1711,  1947.2322],\n",
       "          [-3144.9407, -1485.0723,  1882.6914,  ...,   590.9350,\n",
       "           -1248.7214,   806.9880]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1068.6527,   453.6537, -2823.7739,  ...,  1433.8257,\n",
       "            2354.7148,  2612.2007],\n",
       "          [  712.9173,   118.7017,  -926.3109,  ...,  1354.1694,\n",
       "             360.2656,  1663.5945]],\n",
       " \n",
       "         [[ 1565.4348,  -810.1088,   188.5870,  ...,  1010.5414,\n",
       "            -417.1766,   814.1782],\n",
       "          [  -92.6525, -1011.6904, -2139.6243,  ..., -3194.3889,\n",
       "            -217.9553, -2925.8811]],\n",
       " \n",
       "         [[ 1588.9606,  -649.3470,   255.6078,  ...,  3298.5098,\n",
       "            -368.0341,  1110.7490],\n",
       "          [ -184.6204,   177.0364, -1371.4309,  ...,   309.6882,\n",
       "              -3.7749,  -476.2153]]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor([ 0, 58, 56,  0,  3,  0, 26,  0, 36, 27,  0,  0,  0, 13,  0, 13, 54,  0,\n",
       "         28,  0,  0,  0,  0,  5,  0,  0,  0, 36, 55,  5,  0,  0,  0, 51, 22,  0,\n",
       "         19,  0,  9, 24,  0,  0, 31, 35,  0,  8, 17, 52, 60, 21,  0,  0, 35, 15,\n",
       "         21,  0, 53,  0, 11,  0, 50,  0, 58, 57,  3,  0,  0,  0,  0,  0,  0, 56,\n",
       "          0,  0,  0, 23,  0,  0,  9, 17,  0,  6, 59,  0, 33, 25,  0,  0,  0,  0,\n",
       "          0, 37,  0,  5, 19, 28,  0, 43,  0,  0,  0,  0,  4,  0,  0, 37,  6,  0,\n",
       "         37, 15, 41,  0, 44, 15, 62, 57, 52,  0,  0,  0, 33,  0,  2, 12,  6,  0,\n",
       "         45,  0,  0,  0,  0, 43,  0,  0,  0, 25, 61,  0,  0,  0, 62,  0,  0,  0,\n",
       "          0, 13, 13,  0, 57, 45,  0,  0, 33,  4,  0,  0, 30, 63,  2, 11,  0,  0,\n",
       "         30,  0,  0, 48, 47,  0, 43,  0, 44, 32,  0, 27, 57, 27,  0, 13,  0,  0,\n",
       "          0, 59, 41,  0, 38,  0,  0,  7, 29,  0,  0,  0, 34,  0,  0, 55,  4, 48,\n",
       "          0, 46,  0,  0,  0, 59, 22, 43,  0, 10, 22, 35,  0,  0,  0,  0, 25,  0,\n",
       "         25, 42, 32,  0, 42, 12,  0,  0,  0,  0, 32, 32, 45,  0,  0,  0, 32,  0,\n",
       "          0,  0, 22, 53,  0, 19, 19,  0,  0,  0, 30,  2,  0, 49,  0,  2,  0, 15,\n",
       "         48,  0,  0, 21]),\n",
       " tensor([[-809.9097,  949.9618, -809.9097,  949.9618],\n",
       "         [-362.5710,  447.7780, -487.5289,  708.2578],\n",
       "         [-556.5168,  781.8157,  981.4241, -685.8655],\n",
       "         ...,\n",
       "         [ 753.1874,  -23.8420,  753.1874,  -23.8420],\n",
       "         [ 210.1012, -161.8371,  210.1012, -161.8371],\n",
       "         [ -99.7832, -987.2088,  915.9093,  -58.8253]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data = next(iter(train_loader))\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpAmplitudeModel(nn.Module):\n",
    "    def __init__(self, signal_length=1024, n_bins=64):\n",
    "        super().__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.bin_size = signal_length // n_bins\n",
    "        \n",
    "        # Feature extractor with dilated convolutions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=5, padding=2, dilation=1),\n",
    "            nn.InstanceNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=4, dilation=2),\n",
    "            nn.InstanceNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=8, dilation=4),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Jump probability head\n",
    "        self.jump_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # [batch, 64, 1]\n",
    "            nn.Flatten(),             # [batch, 64]\n",
    "            nn.Linear(64, 1),         # [batch, 1]\n",
    "            nn.Sigmoid()              # [batch, 1]\n",
    "        )\n",
    "        \n",
    "        # Position probability head\n",
    "        self.pos_head = nn.Sequential(\n",
    "            nn.Conv1d(64, n_bins, kernel_size=1),  # [batch, n_bins, signal_length]\n",
    "            nn.AdaptiveAvgPool1d(1),              # [batch, n_bins, 1]\n",
    "            nn.Flatten(start_dim=1),              # [batch, n_bins]\n",
    "            nn.Softmax(dim=1)                     # [batch, n_bins]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.features(x)  # [batch, 64, signal_length]\n",
    "        \n",
    "        # Jump probability\n",
    "        jump_p = self.jump_head(features)  # [batch, 1]\n",
    "        \n",
    "        # Position probabilities\n",
    "        pos_probs = self.pos_head(features)  # [batch, n_bins]\n",
    "        \n",
    "        return jump_p, pos_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X, y_indicator, y_cp, y_amp in dataloader:\n",
    "        X, y_indicator, y_cp, y_amp = X.to(device), y_indicator.to(device), y_cp.to(device), y_amp.to(device)\n",
    "\n",
    "        jump_p, pos_probs = model(X)\n",
    "        pred = (jump_p, pos_probs)\n",
    "\n",
    "        gt = (y_indicator.squeeze(), y_cp.squeeze(), y_amp[:,0], y_amp[:,1], y_amp[:,2], y_amp[:,3])\n",
    "\n",
    "        bin_size = X.size(2) // pos_probs.size(1)  # signal_length // n_bins\n",
    "        loss = criterion(pred, gt, X, bin_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X, y_indicator, y_cp, y_amp in dataloader:\n",
    "        X, y_indicator, y_cp, y_amp = X.to(device), y_indicator.to(device), y_cp.to(device), y_amp.to(device)\n",
    "\n",
    "        jump_p, pos_probs = model(X)\n",
    "        pred = (jump_p, pos_probs)\n",
    "\n",
    "        gt = (y_indicator.squeeze(), y_cp.squeeze(), y_amp[:,0], y_amp[:,1], y_amp[:,2], y_amp[:,3])\n",
    "\n",
    "        bin_size = X.size(2) // pos_probs.size(1)  # signal_length // n_bins\n",
    "        loss = criterion(pred, gt, X, bin_size)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_fn,\n",
    "    num_epochs=20,\n",
    "    plot_loss=True,\n",
    "    scheduler=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model and track performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        train_loader (DataLoader): Training DataLoader.\n",
    "        val_loader (DataLoader): Validation DataLoader.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to run on (CPU/GPU).\n",
    "        amp_loss_fn (Loss): Loss function for amplitude regression.\n",
    "        cp_loss_fn (Loss): Loss function for change point localization.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        plot_loss (bool): Whether to plot loss curves.\n",
    "\n",
    "    Returns:\n",
    "        trained_model (nn.Module): Best trained model.\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_wts = model.state_dict()\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, device, loss_fn\n",
    "        )\n",
    "        val_loss = validate(model, val_loader, device, loss_fn)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            print(scheduler.get_last_lr())\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Training complete. Best Val Loss: {:.4f}\".format(best_val_loss))\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amplitudes(x, pos_probs, bin_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input signal of shape [batch_size, 2, signal_length]\n",
    "        pos_probs: Position probabilities of shape [batch_size, n_bins]\n",
    "        bin_size: Size of each bin (signal_length // n_bins)\n",
    "    Returns:\n",
    "        start_i: [batch_size] (start in-phase amplitude)\n",
    "        start_q: [batch_size] (start quadrature amplitude)\n",
    "        stop_i: [batch_size] (stop in-phase amplitude)\n",
    "        stop_q: [batch_size] (stop quadrature amplitude)\n",
    "    \"\"\"\n",
    "    batch_size, n_bins = pos_probs.shape\n",
    "    signal_length = x.size(2)\n",
    "    device = x.device\n",
    "\n",
    "    # Compute cumulative sums for efficient mean calculation\n",
    "    cumsum_i = torch.cumsum(x[:, 0, :], dim=1)  # [batch_size, signal_length]\n",
    "    cumsum_q = torch.cumsum(x[:, 1, :], dim=1)  # [batch_size, signal_length]\n",
    "\n",
    "    # Precompute all possible split points\n",
    "    split_points = torch.arange(0, n_bins, device=device) * bin_size  # [n_bins]\n",
    "    split_points = split_points.unsqueeze(0).repeat(batch_size, 1)  # [batch_size, n_bins]\n",
    "\n",
    "    # Compute means for all possible splits\n",
    "    means_i = torch.zeros(batch_size, n_bins, 2, device=device)  # [start, stop]\n",
    "    means_q = torch.zeros_like(means_i)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        t = split_points[:, i]  # [batch_size]\n",
    "        t = torch.clamp(t, 1, signal_length - 1)  # Avoid division by zero\n",
    "\n",
    "        # Start means\n",
    "        means_i[:, i, 0] = cumsum_i.gather(1, t.long().unsqueeze(1)).squeeze() / t\n",
    "        means_q[:, i, 0] = cumsum_q.gather(1, t.long().unsqueeze(1)).squeeze() / t\n",
    "\n",
    "        # Stop means\n",
    "        means_i[:, i, 1] = (cumsum_i[:, -1] - cumsum_i.gather(1, t.long().unsqueeze(1)).squeeze()) / (signal_length - t)\n",
    "        means_q[:, i, 1] = (cumsum_q[:, -1] - cumsum_q.gather(1, t.long().unsqueeze(1)).squeeze()) / (signal_length - t)\n",
    "\n",
    "    # Weight by position probabilities\n",
    "    weighted_start_i = (means_i[..., 0] * pos_probs).sum(dim=1)  # [batch_size]\n",
    "    weighted_stop_i = (means_i[..., 1] * pos_probs).sum(dim=1)  # [batch_size] \n",
    "    weighted_start_q = (means_q[..., 0] * pos_probs).sum(dim=1)  # [batch_size]\n",
    "    weighted_stop_q = (means_q[..., 1] * pos_probs).sum(dim=1)  # [batch_size] \n",
    "\n",
    "    return weighted_start_i, weighted_start_q, weighted_stop_i, weighted_stop_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0, amp_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()  # For jump classification\n",
    "        self.ce = nn.CrossEntropyLoss()  # For position prediction\n",
    "        self.amp_loss = nn.SmoothL1Loss()  # For amplitude regression\n",
    "        self.pos_weight = pos_weight\n",
    "        self.amp_weight = amp_weight\n",
    "\n",
    "    def forward(self, pred, gt, x, bin_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Tuple of (jump_p, pos_probs)\n",
    "                - jump_p: [batch_size, 1] (probability of jump)\n",
    "                - pos_probs: [batch_size, n_bins] (probability distribution over positions)\n",
    "            gt: Tuple of (gt_jump, gt_pos, gt_start_i, gt_start_q, gt_stop_i, gt_stop_q)\n",
    "                - gt_jump: [batch_size] (binary labels: 1 if jump exists, 0 otherwise)\n",
    "                - gt_pos: [batch_size] (integer position of jump, in [0, n_bins-1])\n",
    "                - gt_start_i: [batch_size] (start in-phase amplitude)\n",
    "                - gt_start_q: [batch_size] (start quadrature amplitude)\n",
    "                - gt_stop_i: [batch_size] (stop in-phase amplitude)\n",
    "                - gt_stop_q: [batch_size] (stop quadrature amplitude)\n",
    "            x: Input signal of shape [batch_size, 2, signal_length]\n",
    "            bin_size: Size of each bin (signal_length // n_bins)\n",
    "        \"\"\"\n",
    "        jump_p, pos_probs = pred\n",
    "        gt_jump, gt_pos, gt_start_i, gt_start_q, gt_stop_i, gt_stop_q = gt\n",
    "\n",
    "        # Jump classification loss\n",
    "        jump_loss = self.bce(jump_p.squeeze(), gt_jump.float())\n",
    "\n",
    "        # Position loss (only when jump exists)\n",
    "        mask = gt_jump.bool()  # Mask for samples with jumps\n",
    "        if mask.any():\n",
    "            pos_loss = self.ce(pos_probs[mask], gt_pos[mask])\n",
    "        else:\n",
    "            pos_loss = torch.tensor(0.0, device=jump_p.device)\n",
    "\n",
    "        # Amplitude loss\n",
    "        start_i, start_q, stop_i, stop_q = compute_amplitudes(x, pos_probs, bin_size)\n",
    "        \n",
    "        amp_loss = self.amp_loss(\n",
    "            torch.vstack([start_i, start_q, stop_i, stop_q]),\n",
    "            torch.vstack([gt_start_i, gt_start_q, gt_stop_i, gt_stop_q])\n",
    "        )\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = jump_loss + self.pos_weight * pos_loss + self.amp_weight * amp_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0, amp_weight=1.0, entropy_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()  # For jump classification\n",
    "        self.ce = nn.CrossEntropyLoss()  # For position prediction\n",
    "        self.amp_loss = nn.SmoothL1Loss()  # For amplitude regression\n",
    "        self.pos_weight = pos_weight\n",
    "        self.amp_weight = amp_weight\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "    def forward(self, pred, gt, x, bin_size):\n",
    "        jump_p, pos_probs = pred\n",
    "        gt_jump, gt_pos, gt_start_i, gt_start_q, gt_stop_i, gt_stop_q = gt\n",
    "\n",
    "        # Jump classification loss\n",
    "        jump_loss = self.bce(jump_p.squeeze(), gt_jump.float())\n",
    "\n",
    "        # Position loss (only when jump exists)\n",
    "        mask = gt_jump.bool()  # Mask for samples with jumps\n",
    "        if mask.any():\n",
    "            pos_loss = self.ce(pos_probs[mask], gt_pos[mask])\n",
    "        else:\n",
    "            pos_loss = torch.tensor(0.0, device=jump_p.device)\n",
    "\n",
    "        # Amplitude loss\n",
    "        start_i, start_q, stop_i, stop_q = compute_amplitudes(x, pos_probs, bin_size)\n",
    "        amp_loss = self.amp_loss(\n",
    "            torch.vstack([start_i, start_q, stop_i, stop_q]),\n",
    "            torch.vstack([gt_start_i, gt_start_q, gt_stop_i, gt_stop_q])\n",
    "        )\n",
    "\n",
    "        # Entropy regularization (only when jump exists)\n",
    "        entropy = -torch.sum(pos_probs * torch.log(pos_probs + 1e-10), dim=1)  # [batch_size]\n",
    "        entropy_loss = (entropy * gt_jump).mean()  # Apply only to samples with jumps\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = (\n",
    "            jump_loss +\n",
    "            self.pos_weight * pos_loss +\n",
    "            self.amp_weight * amp_loss +\n",
    "            self.entropy_weight * entropy_loss\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 6, 7, 8, 9]]),\n",
       " tensor([[ 0,  1,  3,  6, 10],\n",
       "         [ 5, 11, 18, 26, 35]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2,5)\n",
    "a, torch.cumsum(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor([0, 16, 32, 48, 64])\n",
    "split_points = b.unsqueeze(0).repeat(4, 1)  # [4, n_bins]\n",
    "split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = split_points[:, 0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.clamp(t, 1, 80 - 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize model and loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = JumpAmplitudeModel().to(device)\n",
    "criterion = CustomLoss(pos_weight=1.0, amp_weight=1.0, entropy_weight=0.1)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001]\n",
      "Epoch [1/250]  Train Loss: 120.6440  Val Loss: 121.5906\n",
      "[0.001]\n",
      "Epoch [2/250]  Train Loss: 119.8973  Val Loss: 121.2520\n",
      "[0.001]\n",
      "Epoch [3/250]  Train Loss: 119.4187  Val Loss: 120.6322\n",
      "[0.001]\n",
      "Epoch [4/250]  Train Loss: 118.8217  Val Loss: 120.0860\n",
      "[0.001]\n",
      "Epoch [5/250]  Train Loss: 118.1449  Val Loss: 119.3611\n",
      "[0.001]\n",
      "Epoch [6/250]  Train Loss: 117.6193  Val Loss: 118.7978\n",
      "[0.001]\n",
      "Epoch [7/250]  Train Loss: 116.9599  Val Loss: 118.1432\n",
      "[0.001]\n",
      "Epoch [8/250]  Train Loss: 116.3410  Val Loss: 117.6985\n",
      "[0.001]\n",
      "Epoch [9/250]  Train Loss: 115.8813  Val Loss: 117.4499\n",
      "[0.001]\n",
      "Epoch [10/250]  Train Loss: 115.6626  Val Loss: 116.9651\n",
      "[0.001]\n",
      "Epoch [11/250]  Train Loss: 115.1353  Val Loss: 116.6583\n",
      "[0.001]\n",
      "Epoch [12/250]  Train Loss: 114.9296  Val Loss: 116.2847\n",
      "[0.001]\n",
      "Epoch [13/250]  Train Loss: 114.5934  Val Loss: 115.9765\n",
      "[0.001]\n",
      "Epoch [14/250]  Train Loss: 114.4369  Val Loss: 115.7951\n",
      "[0.001]\n",
      "Epoch [15/250]  Train Loss: 114.1201  Val Loss: 115.4497\n",
      "[0.001]\n",
      "Epoch [16/250]  Train Loss: 114.0159  Val Loss: 115.2886\n",
      "[0.001]\n",
      "Epoch [17/250]  Train Loss: 113.6875  Val Loss: 115.1757\n",
      "[0.001]\n",
      "Epoch [18/250]  Train Loss: 113.6032  Val Loss: 115.0016\n",
      "[0.001]\n",
      "Epoch [19/250]  Train Loss: 113.5303  Val Loss: 114.7057\n",
      "[0.001]\n",
      "Epoch [20/250]  Train Loss: 113.3084  Val Loss: 114.8979\n",
      "[0.001]\n",
      "Epoch [21/250]  Train Loss: 113.0943  Val Loss: 114.4697\n",
      "[0.001]\n",
      "Epoch [22/250]  Train Loss: 113.0238  Val Loss: 114.5657\n",
      "[0.001]\n",
      "Epoch [23/250]  Train Loss: 112.9841  Val Loss: 114.3671\n",
      "[0.001]\n",
      "Epoch [24/250]  Train Loss: 112.8084  Val Loss: 114.1979\n",
      "[0.001]\n",
      "Epoch [25/250]  Train Loss: 112.7543  Val Loss: 114.0176\n",
      "[0.001]\n",
      "Epoch [26/250]  Train Loss: 112.6503  Val Loss: 113.9256\n",
      "[0.001]\n",
      "Epoch [27/250]  Train Loss: 112.5015  Val Loss: 113.9086\n",
      "[0.001]\n",
      "Epoch [28/250]  Train Loss: 112.5101  Val Loss: 113.7754\n",
      "[0.001]\n",
      "Epoch [29/250]  Train Loss: 112.4046  Val Loss: 114.0745\n",
      "[0.001]\n",
      "Epoch [30/250]  Train Loss: 112.4867  Val Loss: 113.8511\n",
      "[0.001]\n",
      "Epoch [31/250]  Train Loss: 112.1964  Val Loss: 113.6042\n",
      "[0.001]\n",
      "Epoch [32/250]  Train Loss: 112.1417  Val Loss: 113.6021\n",
      "[0.001]\n",
      "Epoch [33/250]  Train Loss: 112.0139  Val Loss: 113.5324\n",
      "[0.001]\n",
      "Epoch [34/250]  Train Loss: 112.0390  Val Loss: 113.5609\n",
      "[0.001]\n",
      "Epoch [35/250]  Train Loss: 111.9772  Val Loss: 113.3426\n",
      "[0.001]\n",
      "Epoch [36/250]  Train Loss: 111.9002  Val Loss: 113.3511\n",
      "[0.001]\n",
      "Epoch [37/250]  Train Loss: 111.7017  Val Loss: 113.2771\n",
      "[0.001]\n",
      "Epoch [38/250]  Train Loss: 111.6989  Val Loss: 113.3879\n",
      "[0.001]\n",
      "Epoch [39/250]  Train Loss: 111.7633  Val Loss: 113.1873\n",
      "[0.001]\n",
      "Epoch [40/250]  Train Loss: 111.6397  Val Loss: 113.0192\n",
      "[0.001]\n",
      "Epoch [41/250]  Train Loss: 111.5572  Val Loss: 113.0888\n",
      "[0.001]\n",
      "Epoch [42/250]  Train Loss: 111.4315  Val Loss: 112.9301\n",
      "[0.001]\n",
      "Epoch [43/250]  Train Loss: 111.4261  Val Loss: 112.7737\n",
      "[0.001]\n",
      "Epoch [44/250]  Train Loss: 111.2770  Val Loss: 112.6568\n",
      "[0.001]\n",
      "Epoch [45/250]  Train Loss: 111.2508  Val Loss: 112.5422\n",
      "[0.001]\n",
      "Epoch [46/250]  Train Loss: 111.1294  Val Loss: 112.4334\n",
      "[0.001]\n",
      "Epoch [47/250]  Train Loss: 110.8876  Val Loss: 112.3811\n",
      "[0.001]\n",
      "Epoch [48/250]  Train Loss: 110.8846  Val Loss: 112.3166\n",
      "[0.001]\n",
      "Epoch [49/250]  Train Loss: 110.7285  Val Loss: 111.8761\n",
      "[0.001]\n",
      "Epoch [50/250]  Train Loss: 110.4092  Val Loss: 111.7067\n",
      "[0.001]\n",
      "Epoch [51/250]  Train Loss: 110.1131  Val Loss: 111.8034\n",
      "[0.001]\n",
      "Epoch [52/250]  Train Loss: 109.9032  Val Loss: 111.0256\n",
      "[0.001]\n",
      "Epoch [53/250]  Train Loss: 109.5261  Val Loss: 110.9855\n",
      "[0.001]\n",
      "Epoch [54/250]  Train Loss: 109.1619  Val Loss: 110.4206\n",
      "[0.001]\n",
      "Epoch [55/250]  Train Loss: 108.7432  Val Loss: 109.9844\n",
      "[0.001]\n",
      "Epoch [56/250]  Train Loss: 108.6361  Val Loss: 109.8902\n",
      "[0.001]\n",
      "Epoch [57/250]  Train Loss: 108.1756  Val Loss: 109.2696\n",
      "[0.001]\n",
      "Epoch [58/250]  Train Loss: 107.8860  Val Loss: 108.9594\n",
      "[0.001]\n",
      "Epoch [59/250]  Train Loss: 107.5179  Val Loss: 108.5832\n",
      "[0.001]\n",
      "Epoch [60/250]  Train Loss: 107.2207  Val Loss: 108.5238\n",
      "[0.001]\n",
      "Epoch [61/250]  Train Loss: 107.0020  Val Loss: 108.4217\n",
      "[0.001]\n",
      "Epoch [62/250]  Train Loss: 106.7947  Val Loss: 108.2159\n",
      "[0.001]\n",
      "Epoch [63/250]  Train Loss: 106.4705  Val Loss: 108.0229\n",
      "[0.001]\n",
      "Epoch [64/250]  Train Loss: 106.2333  Val Loss: 107.7168\n",
      "[0.001]\n",
      "Epoch [65/250]  Train Loss: 106.0908  Val Loss: 107.6760\n",
      "[0.001]\n",
      "Epoch [66/250]  Train Loss: 105.8087  Val Loss: 107.2629\n",
      "[0.001]\n",
      "Epoch [67/250]  Train Loss: 105.6981  Val Loss: 106.9046\n",
      "[0.001]\n",
      "Epoch [68/250]  Train Loss: 105.5539  Val Loss: 106.8963\n",
      "[0.001]\n",
      "Epoch [69/250]  Train Loss: 105.3178  Val Loss: 106.8404\n",
      "[0.001]\n",
      "Epoch [70/250]  Train Loss: 104.8760  Val Loss: 105.5689\n",
      "[0.001]\n",
      "Epoch [71/250]  Train Loss: 102.9491  Val Loss: 103.6286\n",
      "[0.001]\n",
      "Epoch [72/250]  Train Loss: 101.7173  Val Loss: 102.8626\n",
      "[0.001]\n",
      "Epoch [73/250]  Train Loss: 100.8698  Val Loss: 102.3504\n",
      "[0.001]\n",
      "Epoch [74/250]  Train Loss: 100.1897  Val Loss: 102.1181\n",
      "[0.001]\n",
      "Epoch [75/250]  Train Loss: 99.8384  Val Loss: 101.6463\n",
      "[0.001]\n",
      "Epoch [76/250]  Train Loss: 99.3804  Val Loss: 100.5546\n",
      "[0.001]\n",
      "Epoch [77/250]  Train Loss: 98.9800  Val Loss: 100.2496\n",
      "[0.001]\n",
      "Epoch [78/250]  Train Loss: 98.7735  Val Loss: 100.3691\n",
      "[0.001]\n",
      "Epoch [79/250]  Train Loss: 98.4618  Val Loss: 99.7580\n",
      "[0.001]\n",
      "Epoch [80/250]  Train Loss: 98.1036  Val Loss: 99.7261\n",
      "[0.001]\n",
      "Epoch [81/250]  Train Loss: 97.9770  Val Loss: 99.6383\n",
      "[0.001]\n",
      "Epoch [82/250]  Train Loss: 97.7398  Val Loss: 99.1582\n",
      "[0.001]\n",
      "Epoch [83/250]  Train Loss: 97.4606  Val Loss: 98.9598\n",
      "[0.001]\n",
      "Epoch [84/250]  Train Loss: 97.1678  Val Loss: 98.4905\n",
      "[0.001]\n",
      "Epoch [85/250]  Train Loss: 96.9988  Val Loss: 98.3880\n",
      "[0.001]\n",
      "Epoch [86/250]  Train Loss: 96.8440  Val Loss: 98.2688\n",
      "[0.001]\n",
      "Epoch [87/250]  Train Loss: 96.7001  Val Loss: 98.0030\n",
      "[0.001]\n",
      "Epoch [88/250]  Train Loss: 96.4960  Val Loss: 98.1094\n",
      "[0.001]\n",
      "Epoch [89/250]  Train Loss: 96.2806  Val Loss: 97.6394\n",
      "[0.001]\n",
      "Epoch [90/250]  Train Loss: 96.1657  Val Loss: 97.8745\n",
      "[0.001]\n",
      "Epoch [91/250]  Train Loss: 96.0869  Val Loss: 97.5457\n",
      "[0.001]\n",
      "Epoch [92/250]  Train Loss: 95.9494  Val Loss: 97.6672\n",
      "[0.001]\n",
      "Epoch [93/250]  Train Loss: 95.6257  Val Loss: 97.2257\n",
      "[0.001]\n",
      "Epoch [94/250]  Train Loss: 95.6735  Val Loss: 97.2482\n",
      "[0.001]\n",
      "Epoch [95/250]  Train Loss: 95.4452  Val Loss: 96.8664\n",
      "[0.001]\n",
      "Epoch [96/250]  Train Loss: 95.3348  Val Loss: 97.1865\n",
      "[0.001]\n",
      "Epoch [97/250]  Train Loss: 95.3535  Val Loss: 97.0104\n",
      "[0.001]\n",
      "Epoch [98/250]  Train Loss: 95.1963  Val Loss: 96.6208\n",
      "[0.001]\n",
      "Epoch [99/250]  Train Loss: 95.0290  Val Loss: 96.6517\n",
      "[0.001]\n",
      "Epoch [100/250]  Train Loss: 94.9275  Val Loss: 96.3177\n",
      "[0.001]\n",
      "Epoch [101/250]  Train Loss: 94.8701  Val Loss: 96.4380\n",
      "[0.001]\n",
      "Epoch [102/250]  Train Loss: 94.7399  Val Loss: 96.1747\n",
      "[0.001]\n",
      "Epoch [103/250]  Train Loss: 94.4053  Val Loss: 95.9585\n",
      "[0.001]\n",
      "Epoch [104/250]  Train Loss: 94.5797  Val Loss: 95.9720\n",
      "[0.001]\n",
      "Epoch [105/250]  Train Loss: 94.3361  Val Loss: 96.0649\n",
      "[0.001]\n",
      "Epoch [106/250]  Train Loss: 94.3303  Val Loss: 95.9993\n",
      "[0.001]\n",
      "Epoch [107/250]  Train Loss: 94.2204  Val Loss: 95.4752\n",
      "[0.001]\n",
      "Epoch [108/250]  Train Loss: 94.1179  Val Loss: 95.7581\n",
      "[0.001]\n",
      "Epoch [109/250]  Train Loss: 94.0016  Val Loss: 96.0099\n",
      "[0.001]\n",
      "Epoch [110/250]  Train Loss: 94.1499  Val Loss: 95.3246\n",
      "[0.001]\n",
      "Epoch [111/250]  Train Loss: 93.9460  Val Loss: 95.4419\n",
      "[0.001]\n",
      "Epoch [112/250]  Train Loss: 93.8128  Val Loss: 95.3802\n",
      "[0.001]\n",
      "Epoch [113/250]  Train Loss: 93.6611  Val Loss: 95.3362\n",
      "[0.001]\n",
      "Epoch [114/250]  Train Loss: 93.6371  Val Loss: 94.9875\n",
      "[0.001]\n",
      "Epoch [115/250]  Train Loss: 93.4861  Val Loss: 94.7743\n",
      "[0.001]\n",
      "Epoch [116/250]  Train Loss: 93.3197  Val Loss: 94.9596\n",
      "[0.001]\n",
      "Epoch [117/250]  Train Loss: 93.4059  Val Loss: 94.7918\n",
      "[0.001]\n",
      "Epoch [118/250]  Train Loss: 93.2454  Val Loss: 94.9520\n",
      "[0.001]\n",
      "Epoch [119/250]  Train Loss: 93.1765  Val Loss: 94.9552\n",
      "[0.001]\n",
      "Epoch [120/250]  Train Loss: 93.2393  Val Loss: 94.5873\n",
      "[0.001]\n",
      "Epoch [121/250]  Train Loss: 92.9499  Val Loss: 94.8734\n",
      "[0.001]\n",
      "Epoch [122/250]  Train Loss: 92.9672  Val Loss: 94.6930\n",
      "[0.001]\n",
      "Epoch [123/250]  Train Loss: 92.6871  Val Loss: 94.3517\n",
      "[0.001]\n",
      "Epoch [124/250]  Train Loss: 92.7682  Val Loss: 94.3284\n",
      "[0.001]\n",
      "Epoch [125/250]  Train Loss: 92.6808  Val Loss: 94.4775\n",
      "[0.001]\n",
      "Epoch [126/250]  Train Loss: 92.6354  Val Loss: 94.0793\n",
      "[0.001]\n",
      "Epoch [127/250]  Train Loss: 92.6018  Val Loss: 94.0569\n",
      "[0.001]\n",
      "Epoch [128/250]  Train Loss: 92.4883  Val Loss: 93.9679\n",
      "[0.001]\n",
      "Epoch [129/250]  Train Loss: 92.4322  Val Loss: 94.1245\n",
      "[0.001]\n",
      "Epoch [130/250]  Train Loss: 92.4531  Val Loss: 93.9299\n",
      "[0.001]\n",
      "Epoch [131/250]  Train Loss: 92.4013  Val Loss: 93.8349\n",
      "[0.001]\n",
      "Epoch [132/250]  Train Loss: 92.1541  Val Loss: 94.1542\n",
      "[0.001]\n",
      "Epoch [133/250]  Train Loss: 92.0824  Val Loss: 93.7316\n",
      "[0.001]\n",
      "Epoch [134/250]  Train Loss: 92.0981  Val Loss: 93.9665\n",
      "[0.001]\n",
      "Epoch [135/250]  Train Loss: 92.0248  Val Loss: 94.1491\n",
      "[0.001]\n",
      "Epoch [136/250]  Train Loss: 92.0266  Val Loss: 93.5612\n",
      "[0.001]\n",
      "Epoch [137/250]  Train Loss: 91.9511  Val Loss: 93.7132\n",
      "[0.001]\n",
      "Epoch [138/250]  Train Loss: 91.7628  Val Loss: 94.3027\n",
      "[0.001]\n",
      "Epoch [139/250]  Train Loss: 91.7358  Val Loss: 93.4685\n",
      "[0.001]\n",
      "Epoch [140/250]  Train Loss: 91.5374  Val Loss: 93.0318\n",
      "[0.001]\n",
      "Epoch [141/250]  Train Loss: 91.4893  Val Loss: 93.3173\n",
      "[0.001]\n",
      "Epoch [142/250]  Train Loss: 91.5329  Val Loss: 93.0033\n",
      "[0.001]\n",
      "Epoch [143/250]  Train Loss: 91.4575  Val Loss: 93.4336\n",
      "[0.001]\n",
      "Epoch [144/250]  Train Loss: 91.4778  Val Loss: 93.0820\n",
      "[0.001]\n",
      "Epoch [145/250]  Train Loss: 91.4128  Val Loss: 93.2311\n",
      "[0.001]\n",
      "Epoch [146/250]  Train Loss: 91.1778  Val Loss: 93.0872\n",
      "[0.001]\n",
      "Epoch [147/250]  Train Loss: 91.1435  Val Loss: 92.8779\n",
      "[0.001]\n",
      "Epoch [148/250]  Train Loss: 91.2256  Val Loss: 93.1527\n",
      "[0.001]\n",
      "Epoch [149/250]  Train Loss: 91.1352  Val Loss: 92.9145\n",
      "[0.001]\n",
      "Epoch [150/250]  Train Loss: 91.0250  Val Loss: 93.0831\n",
      "[0.001]\n",
      "Epoch [151/250]  Train Loss: 91.2160  Val Loss: 92.7403\n",
      "[0.001]\n",
      "Epoch [152/250]  Train Loss: 90.9493  Val Loss: 92.5750\n",
      "[0.001]\n",
      "Epoch [153/250]  Train Loss: 90.9819  Val Loss: 92.8793\n",
      "[0.001]\n",
      "Epoch [154/250]  Train Loss: 90.9273  Val Loss: 92.5662\n",
      "[0.001]\n",
      "Epoch [155/250]  Train Loss: 90.7399  Val Loss: 92.4307\n",
      "[0.001]\n",
      "Epoch [156/250]  Train Loss: 90.6240  Val Loss: 92.5045\n",
      "[0.001]\n",
      "Epoch [157/250]  Train Loss: 90.7737  Val Loss: 92.8970\n",
      "[0.001]\n",
      "Epoch [158/250]  Train Loss: 90.6456  Val Loss: 92.3436\n",
      "[0.001]\n",
      "Epoch [159/250]  Train Loss: 90.5471  Val Loss: 92.4789\n",
      "[0.001]\n",
      "Epoch [160/250]  Train Loss: 90.6040  Val Loss: 92.3085\n",
      "[0.001]\n",
      "Epoch [161/250]  Train Loss: 90.4954  Val Loss: 92.3704\n",
      "[0.001]\n",
      "Epoch [162/250]  Train Loss: 90.5058  Val Loss: 92.3397\n",
      "[0.001]\n",
      "Epoch [163/250]  Train Loss: 90.3688  Val Loss: 91.8852\n",
      "[0.001]\n",
      "Epoch [164/250]  Train Loss: 90.2517  Val Loss: 92.3155\n",
      "[0.001]\n",
      "Epoch [165/250]  Train Loss: 90.1004  Val Loss: 91.7635\n",
      "[0.001]\n",
      "Epoch [166/250]  Train Loss: 90.2809  Val Loss: 92.2355\n",
      "[0.001]\n",
      "Epoch [167/250]  Train Loss: 90.2389  Val Loss: 91.8989\n",
      "[0.001]\n",
      "Epoch [168/250]  Train Loss: 90.0915  Val Loss: 91.9113\n",
      "[0.001]\n",
      "Epoch [169/250]  Train Loss: 90.0887  Val Loss: 91.9712\n",
      "[0.001]\n",
      "Epoch [170/250]  Train Loss: 90.1114  Val Loss: 92.0201\n",
      "[0.001]\n",
      "Epoch [171/250]  Train Loss: 89.8681  Val Loss: 92.0076\n",
      "[0.001]\n",
      "Epoch [172/250]  Train Loss: 89.8981  Val Loss: 91.6324\n",
      "[0.001]\n",
      "Epoch [173/250]  Train Loss: 89.8714  Val Loss: 91.5737\n",
      "[0.001]\n",
      "Epoch [174/250]  Train Loss: 89.8320  Val Loss: 91.7368\n",
      "[0.001]\n",
      "Epoch [175/250]  Train Loss: 89.8165  Val Loss: 91.4731\n",
      "[0.001]\n",
      "Epoch [176/250]  Train Loss: 89.7858  Val Loss: 91.5918\n",
      "[0.001]\n",
      "Epoch [177/250]  Train Loss: 89.6849  Val Loss: 91.4884\n",
      "[0.001]\n",
      "Epoch [178/250]  Train Loss: 89.6334  Val Loss: 91.4145\n",
      "[0.001]\n",
      "Epoch [179/250]  Train Loss: 89.7085  Val Loss: 91.3274\n",
      "[0.001]\n",
      "Epoch [180/250]  Train Loss: 89.6390  Val Loss: 91.3383\n",
      "[0.001]\n",
      "Epoch [181/250]  Train Loss: 89.5391  Val Loss: 91.8019\n",
      "[0.001]\n",
      "Epoch [182/250]  Train Loss: 89.5926  Val Loss: 91.2448\n",
      "[0.001]\n",
      "Epoch [183/250]  Train Loss: 89.5456  Val Loss: 91.3791\n",
      "[0.001]\n",
      "Epoch [184/250]  Train Loss: 89.6275  Val Loss: 91.3432\n",
      "[0.001]\n",
      "Epoch [185/250]  Train Loss: 89.4536  Val Loss: 91.0831\n",
      "[0.001]\n",
      "Epoch [186/250]  Train Loss: 89.5218  Val Loss: 91.2989\n",
      "[0.001]\n",
      "Epoch [187/250]  Train Loss: 89.3723  Val Loss: 91.2675\n",
      "[0.001]\n",
      "Epoch [188/250]  Train Loss: 89.3206  Val Loss: 91.0307\n",
      "[0.001]\n",
      "Epoch [189/250]  Train Loss: 89.2684  Val Loss: 91.0565\n",
      "[0.001]\n",
      "Epoch [190/250]  Train Loss: 89.1562  Val Loss: 91.0946\n",
      "[0.001]\n",
      "Epoch [191/250]  Train Loss: 89.1262  Val Loss: 91.1375\n",
      "[0.001]\n",
      "Epoch [192/250]  Train Loss: 89.2272  Val Loss: 91.1392\n",
      "[0.001]\n",
      "Epoch [193/250]  Train Loss: 89.1970  Val Loss: 91.0089\n",
      "[0.001]\n",
      "Epoch [194/250]  Train Loss: 89.1589  Val Loss: 90.8464\n",
      "[0.001]\n",
      "Epoch [195/250]  Train Loss: 89.0491  Val Loss: 90.8367\n",
      "[0.001]\n",
      "Epoch [196/250]  Train Loss: 88.9827  Val Loss: 90.8210\n",
      "[0.001]\n",
      "Epoch [197/250]  Train Loss: 88.9298  Val Loss: 90.8161\n",
      "[0.001]\n",
      "Epoch [198/250]  Train Loss: 88.9306  Val Loss: 90.9291\n",
      "[0.001]\n",
      "Epoch [199/250]  Train Loss: 88.9956  Val Loss: 91.2123\n",
      "[0.001]\n",
      "Epoch [200/250]  Train Loss: 88.9388  Val Loss: 90.8703\n",
      "[0.001]\n",
      "Epoch [201/250]  Train Loss: 88.8322  Val Loss: 90.6194\n",
      "[0.001]\n",
      "Epoch [202/250]  Train Loss: 88.9429  Val Loss: 90.6652\n",
      "[0.001]\n",
      "Epoch [203/250]  Train Loss: 88.7590  Val Loss: 90.8666\n",
      "[0.001]\n",
      "Epoch [204/250]  Train Loss: 88.7278  Val Loss: 90.6856\n",
      "[0.001]\n",
      "Epoch [205/250]  Train Loss: 88.6930  Val Loss: 90.5684\n",
      "[0.001]\n",
      "Epoch [206/250]  Train Loss: 88.7320  Val Loss: 90.6227\n",
      "[0.001]\n",
      "Epoch [207/250]  Train Loss: 88.6995  Val Loss: 90.6141\n",
      "[0.001]\n",
      "Epoch [208/250]  Train Loss: 88.6735  Val Loss: 90.3621\n",
      "[0.001]\n",
      "Epoch [209/250]  Train Loss: 88.6181  Val Loss: 90.5654\n",
      "[0.001]\n",
      "Epoch [210/250]  Train Loss: 88.5910  Val Loss: 90.8941\n",
      "[0.001]\n",
      "Epoch [211/250]  Train Loss: 88.5798  Val Loss: 90.4414\n",
      "[0.001]\n",
      "Epoch [212/250]  Train Loss: 88.5552  Val Loss: 90.4289\n",
      "[0.001]\n",
      "Epoch [213/250]  Train Loss: 88.4896  Val Loss: 90.5410\n",
      "[0.001]\n",
      "Epoch [214/250]  Train Loss: 88.3441  Val Loss: 90.3509\n",
      "[0.001]\n",
      "Epoch [215/250]  Train Loss: 88.4449  Val Loss: 90.3961\n",
      "[0.001]\n",
      "Epoch [216/250]  Train Loss: 88.4141  Val Loss: 90.2201\n",
      "[0.001]\n",
      "Epoch [217/250]  Train Loss: 88.2880  Val Loss: 90.4007\n",
      "[0.001]\n",
      "Epoch [218/250]  Train Loss: 88.2773  Val Loss: 90.0988\n",
      "[0.001]\n",
      "Epoch [219/250]  Train Loss: 88.2575  Val Loss: 90.3038\n",
      "[0.001]\n",
      "Epoch [220/250]  Train Loss: 88.2445  Val Loss: 90.3630\n",
      "[0.001]\n",
      "Epoch [221/250]  Train Loss: 88.2892  Val Loss: 90.0524\n",
      "[0.001]\n",
      "Epoch [222/250]  Train Loss: 88.2211  Val Loss: 90.0977\n",
      "[0.001]\n",
      "Epoch [223/250]  Train Loss: 88.1906  Val Loss: 90.3649\n",
      "[0.001]\n",
      "Epoch [224/250]  Train Loss: 88.1893  Val Loss: 90.7690\n",
      "[0.001]\n",
      "Epoch [225/250]  Train Loss: 88.1799  Val Loss: 90.2324\n",
      "[0.001]\n",
      "Epoch [226/250]  Train Loss: 88.0962  Val Loss: 90.4174\n",
      "[0.001]\n",
      "Epoch [227/250]  Train Loss: 88.0123  Val Loss: 89.9627\n",
      "[0.001]\n",
      "Epoch [228/250]  Train Loss: 87.9853  Val Loss: 89.9714\n",
      "[0.001]\n",
      "Epoch [229/250]  Train Loss: 87.9951  Val Loss: 90.0208\n",
      "[0.001]\n",
      "Epoch [230/250]  Train Loss: 87.9824  Val Loss: 90.2306\n",
      "[0.001]\n",
      "Epoch [231/250]  Train Loss: 87.9510  Val Loss: 89.9964\n",
      "[0.001]\n",
      "Epoch [232/250]  Train Loss: 88.0120  Val Loss: 89.8741\n",
      "[0.001]\n",
      "Epoch [233/250]  Train Loss: 87.8716  Val Loss: 90.0322\n",
      "[0.001]\n",
      "Epoch [234/250]  Train Loss: 87.8740  Val Loss: 89.9859\n",
      "[0.001]\n",
      "Epoch [235/250]  Train Loss: 87.9331  Val Loss: 90.0873\n",
      "[0.001]\n",
      "Epoch [236/250]  Train Loss: 87.7346  Val Loss: 89.8292\n",
      "[0.001]\n",
      "Epoch [237/250]  Train Loss: 87.7604  Val Loss: 89.9397\n",
      "[0.001]\n",
      "Epoch [238/250]  Train Loss: 87.8008  Val Loss: 89.8941\n",
      "[0.001]\n",
      "Epoch [239/250]  Train Loss: 87.7838  Val Loss: 89.9202\n",
      "[0.001]\n",
      "Epoch [240/250]  Train Loss: 87.7226  Val Loss: 89.8729\n",
      "[0.001]\n",
      "Epoch [241/250]  Train Loss: 87.6845  Val Loss: 90.0808\n",
      "[0.001]\n",
      "Epoch [242/250]  Train Loss: 87.7890  Val Loss: 89.9939\n",
      "[0.001]\n",
      "Epoch [243/250]  Train Loss: 87.6658  Val Loss: 89.6696\n",
      "[0.001]\n",
      "Epoch [244/250]  Train Loss: 87.6069  Val Loss: 89.6546\n",
      "[0.001]\n",
      "Epoch [245/250]  Train Loss: 87.6612  Val Loss: 89.7986\n",
      "[0.001]\n",
      "Epoch [246/250]  Train Loss: 87.7326  Val Loss: 89.8012\n",
      "[0.001]\n",
      "Epoch [247/250]  Train Loss: 87.6300  Val Loss: 90.2431\n",
      "[0.001]\n",
      "Epoch [248/250]  Train Loss: 87.4831  Val Loss: 89.8505\n",
      "[0.001]\n",
      "Epoch [249/250]  Train Loss: 87.5307  Val Loss: 89.6421\n",
      "[0.001]\n",
      "Epoch [250/250]  Train Loss: 87.5378  Val Loss: 89.5719\n",
      "Training complete. Best Val Loss: 89.5719\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        criterion,\n",
    "        num_epochs=250,\n",
    "        scheduler=scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 2102.2769,   407.7083,   170.0623,  ...,  -898.3487,\n",
       "           -2779.2090,  -816.0172],\n",
       "          [-2056.9741,  2940.7517,  1271.7192,  ..., -1243.4318,\n",
       "           -2281.5813, -2420.2747]],\n",
       " \n",
       "         [[  -12.9912,  -236.6904,   -77.3014,  ..., -1330.9393,\n",
       "            -317.2576,   305.1830],\n",
       "          [  567.3638, -3496.4246,   -89.3775,  ...,  -790.9786,\n",
       "           -2254.7686, -1884.8403]],\n",
       " \n",
       "         [[ 5115.2349,   499.6626,   948.3342,  ...,  -529.3042,\n",
       "             712.9390,  1056.0459],\n",
       "          [  732.2154,  1106.1027,  3165.0308,  ...,   296.9810,\n",
       "             508.7326,   -70.4058]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  172.2278,   223.4360, -1102.0370,  ...,   328.7923,\n",
       "            -100.4184, -2901.3333],\n",
       "          [  429.5121,  2465.8386, -1451.0809,  ...,   360.3534,\n",
       "            3942.1348,   131.3292]],\n",
       " \n",
       "         [[  -60.4660, -1853.9871, -2994.9580,  ...,  2161.3457,\n",
       "            2394.0864, -2965.7480],\n",
       "          [-1313.1956,  -321.1489, -4373.6396,  ...,     7.8354,\n",
       "            1946.0480,  2086.9888]],\n",
       " \n",
       "         [[ 2769.6006,   383.2892,  1463.9867,  ...,   686.3732,\n",
       "             898.8889,  1053.2865],\n",
       "          [ 1754.6786,   560.2713,  1306.9143,  ..., -3923.4768,\n",
       "           -4232.8140,   836.0437]]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([19, 56,  0,  0,  0,  0,  0,  0,  0,  0, 29,  0,  0, 22,  0, 61,  0,  0,\n",
       "          0, 53,  4, 13,  0,  0, 35, 11,  0,  0, 33, 24, 24,  0, 12, 49,  0, 42,\n",
       "         23,  0,  3, 29,  0, 57, 18, 31,  0,  0, 57,  0,  0, 14,  0, 15, 58, 20,\n",
       "         40,  0,  0,  3, 17,  0,  0,  0, 52,  0, 34,  0,  0,  0,  0,  5, 39,  0,\n",
       "         15,  0, 63,  0,  0,  0, 56, 40,  0, 40,  0,  0, 26, 34, 37,  0, 58, 32,\n",
       "          0, 23,  0,  0, 58,  0,  0,  1,  0,  0,  0, 14, 36,  0,  0,  0,  5,  0,\n",
       "         48, 50, 24, 22,  0,  0, 23,  0,  0,  0,  0,  0, 63,  0,  0,  2, 44,  0,\n",
       "         18,  0,  0,  0,  0, 31,  0, 46, 59,  0,  6, 63, 47,  0,  0,  9, 32,  0,\n",
       "          0,  0, 58,  0, 28,  0,  0, 11,  0, 22, 25,  0,  0, 33, 16,  0,  6, 24,\n",
       "          0,  0,  0, 11,  9,  0, 12, 55,  0, 56, 43,  0,  0, 10, 32,  0,  4,  0,\n",
       "         49, 63, 28, 54, 51, 45,  0,  0,  4, 30,  0,  0, 36,  0, 53,  0,  0,  0,\n",
       "          0, 48,  0, 32,  7,  5,  0,  1,  0, 36,  0,  0,  0,  0,  0,  0,  0, 24,\n",
       "          0,  0,  0, 41, 39,  0,  0,  0, 57,  0,  0, 43,  0,  0,  0, 15,  6,  0,\n",
       "          7, 45, 58, 14,  0,  0, 28,  0,  0, 30,  0, 38,  1,  0,  0, 25,  0,  9,\n",
       "          0,  0, 49, 58]),\n",
       " tensor([[ 153.7832,  163.4942, -922.1494, -952.9467],\n",
       "         [-399.5777, -869.5340,  394.2289,  136.9625],\n",
       "         [ 445.8357,  -15.3292,  445.8357,  -15.3292],\n",
       "         ...,\n",
       "         [-454.0664,  364.9399, -481.5957,  331.9081],\n",
       "         [ -56.7942, -234.2085, -579.2318, -156.2706],\n",
       "         [  17.9769,  196.2753,  354.4303, -854.1609]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dummy = next(iter(val_loader))\n",
    "test_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 153.7832,  163.4942, -922.1494, -952.9467],\n",
       "        [-399.5777, -869.5340,  394.2289,  136.9625],\n",
       "        [ 445.8357,  -15.3292,  445.8357,  -15.3292],\n",
       "        ...,\n",
       "        [-454.0664,  364.9399, -481.5957,  331.9081],\n",
       "        [ -56.7942, -234.2085, -579.2318, -156.2706],\n",
       "        [  17.9769,  196.2753,  354.4303, -854.1609]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dummy[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.4531, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "loss(test_dummy[1], torch.round(pred[0]).to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9736e+02,  0.0000e+00,  0.0000e+00,  6.0725e+02,  8.3568e+02,\n",
       "         5.8925e+02,  1.9452e+02, -8.4051e+02, -7.3593e+02, -3.0441e+02,\n",
       "        -5.6188e+02,  4.9607e+02, -5.4266e+02, -7.6442e+01, -4.9627e+02,\n",
       "        -8.6523e+02, -6.1714e+01, -9.7069e+02, -2.0469e+01,  5.9362e+02,\n",
       "         0.0000e+00,  6.1139e+02,  1.0162e+03,  0.0000e+00,  3.5237e+01,\n",
       "        -5.2420e+02, -3.5259e+02,  0.0000e+00,  1.0360e+03,  1.8568e+02,\n",
       "        -1.4446e+02,  4.5778e+02, -7.7671e+02,  7.8626e+02,  4.4416e+02,\n",
       "         0.0000e+00,  0.0000e+00, -3.0204e+02,  0.0000e+00,  6.1407e+02,\n",
       "        -8.4785e+02,  5.8763e+02, -6.6218e+02, -5.8633e+02,  7.3092e+02,\n",
       "         7.2447e+02,  2.2453e+02,  5.3550e+02, -7.0271e+02,  2.3558e+02,\n",
       "        -9.7118e+02,  0.0000e+00,  0.0000e+00, -7.1487e+02,  3.0978e+02,\n",
       "         3.3180e+02, -1.6380e+02,  1.2399e+02,  0.0000e+00,  3.9770e+02,\n",
       "        -7.7099e+02, -6.6387e+02,  3.6458e+02, -6.7718e+01, -5.2847e+02,\n",
       "        -6.1017e+01,  2.3295e+02,  0.0000e+00, -4.9905e+02, -3.5434e+02,\n",
       "         0.0000e+00,  0.0000e+00, -3.5885e+02,  5.9121e+02,  0.0000e+00,\n",
       "        -4.8636e+02,  3.7884e-01,  6.2353e+02,  0.0000e+00,  7.1324e+02,\n",
       "         9.0348e+02,  0.0000e+00,  0.0000e+00, -7.4495e+02, -2.1375e+02,\n",
       "        -2.2578e+02, -1.4834e+02, -8.0980e+02,  1.2089e+02,  0.0000e+00,\n",
       "        -4.6853e+02, -1.7243e+02,  1.4450e+02,  9.2092e+02,  0.0000e+00,\n",
       "         4.0991e+02,  0.0000e+00,  8.7902e+02, -7.3379e+01,  4.9072e+02,\n",
       "         8.7573e+02,  4.8669e+02,  2.4144e+02,  6.1404e+02, -2.2750e+02,\n",
       "         0.0000e+00,  4.7407e+02, -4.4379e+02,  6.2221e+02,  0.0000e+00,\n",
       "        -1.4680e+02,  3.3632e+02,  1.0856e+02,  2.4721e+02,  1.9545e+01,\n",
       "         3.3788e+02, -1.3004e+02,  2.9555e+02,  6.1286e+02, -7.2956e+02,\n",
       "         9.5771e+02,  9.9858e+02,  9.1323e+02,  0.0000e+00, -3.3728e+02,\n",
       "         3.1463e+02, -8.5670e+02,  0.0000e+00,  0.0000e+00,  6.6061e+02,\n",
       "        -4.8077e+02,  1.6077e+02,  5.5344e+02,  2.0676e+02,  0.0000e+00,\n",
       "        -3.9850e+02, -7.6507e+02, -5.3903e+02,  0.0000e+00,  1.0459e+03,\n",
       "        -3.5812e+02,  3.2920e+02, -9.0964e+02,  0.0000e+00, -1.0090e+03,\n",
       "        -3.2630e+02,  0.0000e+00, -8.2431e+02, -8.0718e+02,  1.2741e+01,\n",
       "         0.0000e+00,  0.0000e+00, -6.8389e+02,  0.0000e+00,  5.7075e+02,\n",
       "        -9.2080e+02,  0.0000e+00, -6.3532e+02,  3.5553e+02,  0.0000e+00,\n",
       "        -1.4493e+02,  2.5377e+02, -5.3136e+02, -8.3029e+02,  0.0000e+00,\n",
       "         8.0691e+02,  0.0000e+00,  6.3382e+02,  0.0000e+00,  0.0000e+00,\n",
       "        -5.9974e+02,  0.0000e+00,  9.0268e+02, -5.2170e+02,  2.6483e+02,\n",
       "         0.0000e+00,  1.0202e+03,  2.0200e+02,  0.0000e+00, -7.4939e+02,\n",
       "         2.1836e+01, -7.1019e+02,  1.8869e+02,  8.2147e+02,  5.0899e+02,\n",
       "         0.0000e+00, -6.2842e+02,  0.0000e+00,  0.0000e+00,  5.7362e+02,\n",
       "        -9.2705e+02, -1.0286e+02,  0.0000e+00,  0.0000e+00, -4.8568e+02,\n",
       "         2.3867e+02,  8.4393e+02,  5.3679e+02, -1.9655e+02,  0.0000e+00,\n",
       "         3.0862e+02, -4.1853e+02, -6.8654e+02, -8.4202e+02,  7.5353e+02,\n",
       "         8.3430e+02,  0.0000e+00, -4.2986e+01,  8.8490e+02,  2.8052e+02,\n",
       "        -3.9544e+02, -1.1324e+02,  6.1102e+02, -1.2806e+02,  0.0000e+00,\n",
       "        -6.5429e+02,  6.4760e+02, -3.3525e+02, -5.9099e+02, -2.3975e+02,\n",
       "         0.0000e+00, -9.6412e+02,  0.0000e+00, -3.6480e+02,  0.0000e+00,\n",
       "         7.6186e+02,  0.0000e+00,  1.6886e+02,  1.0376e+03, -6.4074e+02,\n",
       "         1.8513e+02, -7.0876e+02, -4.1430e+02,  4.3238e+02,  6.4851e+01,\n",
       "         9.6674e+02,  0.0000e+00,  7.9963e+02, -7.5091e+02,  8.3661e+02,\n",
       "         2.4387e+02,  0.0000e+00,  3.9109e+02, -8.2756e+02,  4.0512e+01,\n",
       "        -1.0766e+02,  1.5139e+02, -8.7420e+02,  5.1480e+01,  3.1895e+00,\n",
       "         7.4406e+02, -1.8898e+01,  0.0000e+00, -4.9883e+02,  0.0000e+00,\n",
       "         0.0000e+00], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_amplitudes(test_dummy[0], torch.round(pred[1]).to(\"cpu\"), 16)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.]], device='cuda:0', grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9900],\n",
       "         [0.9899],\n",
       "         [0.5114],\n",
       "         [0.3468],\n",
       "         [0.1166],\n",
       "         [0.1704],\n",
       "         [0.3413],\n",
       "         [0.3289],\n",
       "         [0.3968],\n",
       "         [0.2226],\n",
       "         [0.4530],\n",
       "         [0.4353],\n",
       "         [0.2728],\n",
       "         [0.8959],\n",
       "         [0.1723],\n",
       "         [0.9839],\n",
       "         [0.3319],\n",
       "         [0.3004],\n",
       "         [0.1182],\n",
       "         [0.7793],\n",
       "         [0.3667],\n",
       "         [0.9968],\n",
       "         [0.1966],\n",
       "         [0.3417],\n",
       "         [0.9562],\n",
       "         [0.9952],\n",
       "         [0.3046],\n",
       "         [0.2836],\n",
       "         [0.8747],\n",
       "         [0.9844],\n",
       "         [0.9899],\n",
       "         [0.2014],\n",
       "         [0.9642],\n",
       "         [0.9523],\n",
       "         [0.2203],\n",
       "         [0.8418],\n",
       "         [0.5123],\n",
       "         [0.2307],\n",
       "         [0.5546],\n",
       "         [0.3836],\n",
       "         [0.1347],\n",
       "         [0.2795],\n",
       "         [0.9943],\n",
       "         [0.9583],\n",
       "         [0.1260],\n",
       "         [0.3118],\n",
       "         [0.9726],\n",
       "         [0.2525],\n",
       "         [0.1803],\n",
       "         [0.2999],\n",
       "         [0.2714],\n",
       "         [0.5957],\n",
       "         [0.9897],\n",
       "         [0.9922],\n",
       "         [0.8820],\n",
       "         [0.3183],\n",
       "         [0.2517],\n",
       "         [0.2183],\n",
       "         [0.9862],\n",
       "         [0.2391],\n",
       "         [0.1317],\n",
       "         [0.2554],\n",
       "         [0.2462],\n",
       "         [0.2822],\n",
       "         [0.9326],\n",
       "         [0.3493],\n",
       "         [0.3576],\n",
       "         [0.8291],\n",
       "         [0.1757],\n",
       "         [0.9942],\n",
       "         [0.4009],\n",
       "         [0.6855],\n",
       "         [0.9876],\n",
       "         [0.2441],\n",
       "         [0.1904],\n",
       "         [0.2325],\n",
       "         [0.3634],\n",
       "         [0.2307],\n",
       "         [0.9857],\n",
       "         [0.9867],\n",
       "         [0.2190],\n",
       "         [0.9956],\n",
       "         [0.5603],\n",
       "         [0.2349],\n",
       "         [0.9346],\n",
       "         [0.9444],\n",
       "         [0.9059],\n",
       "         [0.2042],\n",
       "         [0.9911],\n",
       "         [0.7800],\n",
       "         [0.2552],\n",
       "         [0.9880],\n",
       "         [0.1839],\n",
       "         [0.2565],\n",
       "         [0.8257],\n",
       "         [0.2343],\n",
       "         [0.4309],\n",
       "         [0.2060],\n",
       "         [0.2128],\n",
       "         [0.2705],\n",
       "         [0.2669],\n",
       "         [0.9920],\n",
       "         [0.9848],\n",
       "         [0.2062],\n",
       "         [0.2260],\n",
       "         [0.4649],\n",
       "         [0.2138],\n",
       "         [0.3882],\n",
       "         [0.9946],\n",
       "         [0.7778],\n",
       "         [0.2652],\n",
       "         [0.9755],\n",
       "         [0.4406],\n",
       "         [0.4256],\n",
       "         [0.5101],\n",
       "         [0.1404],\n",
       "         [0.1638],\n",
       "         [0.2095],\n",
       "         [0.2150],\n",
       "         [0.2993],\n",
       "         [0.1273],\n",
       "         [0.1926],\n",
       "         [0.1853],\n",
       "         [0.5349],\n",
       "         [0.2169],\n",
       "         [0.4227],\n",
       "         [0.2253],\n",
       "         [0.7498],\n",
       "         [0.2263],\n",
       "         [0.1558],\n",
       "         [0.3817],\n",
       "         [0.9143],\n",
       "         [0.1844],\n",
       "         [0.9314],\n",
       "         [0.9338],\n",
       "         [0.2102],\n",
       "         [0.9955],\n",
       "         [0.3497],\n",
       "         [0.4273],\n",
       "         [0.2952],\n",
       "         [0.1607],\n",
       "         [0.5218],\n",
       "         [0.8906],\n",
       "         [0.5815],\n",
       "         [0.2808],\n",
       "         [0.3387],\n",
       "         [0.9033],\n",
       "         [0.1841],\n",
       "         [0.8409],\n",
       "         [0.2011],\n",
       "         [0.2767],\n",
       "         [0.8820],\n",
       "         [0.2672],\n",
       "         [0.2247],\n",
       "         [0.9891],\n",
       "         [0.3732],\n",
       "         [0.1402],\n",
       "         [0.2317],\n",
       "         [0.9920],\n",
       "         [0.4881],\n",
       "         [0.3867],\n",
       "         [0.9318],\n",
       "         [0.3754],\n",
       "         [0.3681],\n",
       "         [0.4302],\n",
       "         [0.9811],\n",
       "         [0.7085],\n",
       "         [0.3525],\n",
       "         [0.8723],\n",
       "         [0.9710],\n",
       "         [0.4299],\n",
       "         [0.9914],\n",
       "         [0.8949],\n",
       "         [0.1707],\n",
       "         [0.1790],\n",
       "         [0.6100],\n",
       "         [0.9584],\n",
       "         [0.1753],\n",
       "         [0.8300],\n",
       "         [0.2822],\n",
       "         [0.9966],\n",
       "         [0.1218],\n",
       "         [0.8342],\n",
       "         [0.9467],\n",
       "         [0.9951],\n",
       "         [0.8816],\n",
       "         [0.1926],\n",
       "         [0.6252],\n",
       "         [0.3858],\n",
       "         [0.9480],\n",
       "         [0.2804],\n",
       "         [0.2947],\n",
       "         [0.8823],\n",
       "         [0.3260],\n",
       "         [0.9672],\n",
       "         [0.4467],\n",
       "         [0.1636],\n",
       "         [0.1414],\n",
       "         [0.1631],\n",
       "         [0.7355],\n",
       "         [0.1626],\n",
       "         [0.9763],\n",
       "         [0.9660],\n",
       "         [0.9945],\n",
       "         [0.2841],\n",
       "         [0.9731],\n",
       "         [0.3014],\n",
       "         [0.9823],\n",
       "         [0.2848],\n",
       "         [0.1884],\n",
       "         [0.1750],\n",
       "         [0.1501],\n",
       "         [0.2440],\n",
       "         [0.4380],\n",
       "         [0.2356],\n",
       "         [0.6453],\n",
       "         [0.2544],\n",
       "         [0.2208],\n",
       "         [0.2674],\n",
       "         [0.1319],\n",
       "         [0.6172],\n",
       "         [0.3400],\n",
       "         [0.6116],\n",
       "         [0.1978],\n",
       "         [0.5789],\n",
       "         [0.2849],\n",
       "         [0.3128],\n",
       "         [0.9950],\n",
       "         [0.2542],\n",
       "         [0.2768],\n",
       "         [0.2334],\n",
       "         [0.9745],\n",
       "         [0.2708],\n",
       "         [0.2702],\n",
       "         [0.9810],\n",
       "         [0.1892],\n",
       "         [0.9379],\n",
       "         [0.6265],\n",
       "         [0.2951],\n",
       "         [0.1573],\n",
       "         [0.9707],\n",
       "         [0.2210],\n",
       "         [0.2146],\n",
       "         [0.5182],\n",
       "         [0.3006],\n",
       "         [0.4841],\n",
       "         [0.2584],\n",
       "         [0.2768],\n",
       "         [0.3280],\n",
       "         [0.8563],\n",
       "         [0.2800],\n",
       "         [0.9938],\n",
       "         [0.3750],\n",
       "         [0.4082],\n",
       "         [0.4978],\n",
       "         [0.9628]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " tensor([[1.3452e-11, 1.9038e-11, 1.8453e-11,  ..., 2.0286e-04, 5.1972e-05,\n",
       "          6.4621e-13],\n",
       "         [4.6986e-10, 9.3748e-10, 1.1605e-09,  ..., 2.3230e-01, 9.9863e-02,\n",
       "          6.6741e-11],\n",
       "         [8.3182e-11, 1.1263e-10, 1.3460e-10,  ..., 2.6584e-02, 1.6642e-02,\n",
       "          9.2556e-12],\n",
       "         ...,\n",
       "         [2.0614e-11, 2.3441e-11, 2.8721e-11,  ..., 1.3770e-02, 8.1310e-03,\n",
       "          2.2086e-12],\n",
       "         [8.7225e-11, 1.1091e-10, 1.3475e-10,  ..., 3.3668e-02, 2.2730e-02,\n",
       "          1.0619e-11],\n",
       "         [3.0114e-10, 5.0319e-10, 6.8290e-10,  ..., 3.7551e-01, 2.3151e-01,\n",
       "          6.6149e-11]], device='cuda:0', grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = trained_model(test_dummy[0].to(device))\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
