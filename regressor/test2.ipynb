{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datagen import DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(num_samples=1000, signal_length=100, seed=None):\n",
    "    generator = DataGenerator(signal_length=signal_length, seed=seed)\n",
    "\n",
    "    # Generate full dataset\n",
    "    X_jump, (y_amp_jump, y_cp_jump) = generator.generate_data(num_samples // 2, jump=True)\n",
    "    y_jump_indicator = torch.ones_like(y_cp_jump)\n",
    "    X_const, (y_amp_const, y_cp_const) = generator.generate_data(num_samples // 2, jump=False)\n",
    "    y_const_indicator = torch.zeros_like(y_cp_const)\n",
    "\n",
    "    # Convert everything to float32 explicitly\n",
    "    X = torch.cat([X_jump, X_const], dim=0).float()\n",
    "    y_amp = torch.cat([y_amp_jump, y_amp_const], dim=0).float()\n",
    "    y_cp = torch.cat([y_cp_jump, y_cp_const], dim=0).float().unsqueeze(1)\n",
    "    y_indicator = torch.cat([y_jump_indicator, y_const_indicator], dim=0).float().unsqueeze(1)\n",
    "    print(y_amp.shape, y_cp.shape, y_indicator.shape)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    indices = torch.randperm(num_samples)\n",
    "    X, y_amp, y_cp, y_indicator = X[indices], y_amp[indices], y_cp[indices], y_indicator[indices]\n",
    "\n",
    "    return X, y_indicator, y_cp, y_amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ground_truth(jump_positions, n_bins=64, signal_length=1024):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        jump_positions: List of jump positions (in samples) for each signal. Use -1 for no jump.\n",
    "        n_bins: Number of bins for position prediction.\n",
    "        signal_length: Length of the input signal.\n",
    "    Returns:\n",
    "        gt_pos: Tensor of shape [batch_size] (bin index of jump, or 0 if no jump).\n",
    "    \"\"\"\n",
    "    batch_size = len(jump_positions)\n",
    "    gt_pos = torch.zeros(batch_size, dtype=torch.long)\n",
    "    \n",
    "    bin_size = signal_length // n_bins\n",
    "    \n",
    "    for i, pos in enumerate(jump_positions):\n",
    "        if pos >= 0:  # Jump exists\n",
    "            gt_pos[i] = min(pos // bin_size, n_bins - 1)  # Ensure bin index is within range\n",
    "        else:  # No jump\n",
    "            gt_pos[i] = 0  # Irrelevant, since loss will be masked\n",
    "    \n",
    "    return gt_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders(num_samples=1000, signal_length=1024, batch_size=32, train_ratio=0.8, n_bins=64, seed=None):\n",
    "    \n",
    "    X, y_indicator, y_cp, y_amp = prepare_data(num_samples, signal_length, seed)\n",
    "\n",
    "    y_cp = prepare_ground_truth(y_cp, n_bins, signal_length)\n",
    "\n",
    "    num_samples = X.shape[0]\n",
    "    # Train/validation split\n",
    "    train_size = int(train_ratio * num_samples)\n",
    "    val_size = num_samples - train_size\n",
    "    \n",
    "    X_train = X[:train_size]\n",
    "    X_val = X[train_size:]\n",
    "    y_indicator_train = y_indicator[:train_size]\n",
    "    y_indicator_val = y_indicator[train_size:]\n",
    "    y_cp_train = y_cp[:train_size]\n",
    "    y_cp_val = y_cp[train_size:]\n",
    "    y_amp_train = y_amp[:train_size]\n",
    "    y_amp_val = y_amp[train_size:]\n",
    "\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(X_train, y_indicator_train, y_cp_train, y_amp_train)\n",
    "    val_dataset = TensorDataset(X_val, y_indicator_val, y_cp_val, y_amp_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 4]) torch.Size([50000, 1]) torch.Size([50000, 1])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = prepare_dataloaders(batch_size=256, signal_length=1024, seed=42, num_samples=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[  128.9844, -2362.7139, -1459.9290,  ..., -2437.4631,\n",
       "            -418.7927, -1157.3844],\n",
       "          [ 1056.9888,  -727.7728,  2172.7349,  ...,    67.7678,\n",
       "             858.4960,  2347.1743]],\n",
       " \n",
       "         [[-2390.9436, -1202.0691, -1325.5686,  ..., -1055.2358,\n",
       "            -599.0897, -1452.9836],\n",
       "          [  618.1628,  2084.3911,  2036.7823,  ...,   720.4336,\n",
       "            1206.6832, -1961.8562]],\n",
       " \n",
       "         [[-3103.1084,    10.3209,   225.3537,  ...,  1528.3220,\n",
       "            -596.1711,  1947.2322],\n",
       "          [-3144.9407, -1485.0723,  1882.6914,  ...,   590.9350,\n",
       "           -1248.7214,   806.9880]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1068.6527,   453.6537, -2823.7739,  ...,  1433.8257,\n",
       "            2354.7148,  2612.2007],\n",
       "          [  712.9173,   118.7017,  -926.3109,  ...,  1354.1694,\n",
       "             360.2656,  1663.5945]],\n",
       " \n",
       "         [[ 1565.4348,  -810.1088,   188.5870,  ...,  1010.5414,\n",
       "            -417.1766,   814.1782],\n",
       "          [  -92.6525, -1011.6904, -2139.6243,  ..., -3194.3889,\n",
       "            -217.9553, -2925.8811]],\n",
       " \n",
       "         [[ 1588.9606,  -649.3470,   255.6078,  ...,  3298.5098,\n",
       "            -368.0341,  1110.7490],\n",
       "          [ -184.6204,   177.0364, -1371.4309,  ...,   309.6882,\n",
       "              -3.7749,  -476.2153]]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]),\n",
       " tensor([ 0, 58, 56,  0,  3,  0, 26,  0, 36, 27,  0,  0,  0, 13,  0, 13, 54,  0,\n",
       "         28,  0,  0,  0,  0,  5,  0,  0,  0, 36, 55,  5,  0,  0,  0, 51, 22,  0,\n",
       "         19,  0,  9, 24,  0,  0, 31, 35,  0,  8, 17, 52, 60, 21,  0,  0, 35, 15,\n",
       "         21,  0, 53,  0, 11,  0, 50,  0, 58, 57,  3,  0,  0,  0,  0,  0,  0, 56,\n",
       "          0,  0,  0, 23,  0,  0,  9, 17,  0,  6, 59,  0, 33, 25,  0,  0,  0,  0,\n",
       "          0, 37,  0,  5, 19, 28,  0, 43,  0,  0,  0,  0,  4,  0,  0, 37,  6,  0,\n",
       "         37, 15, 41,  0, 44, 15, 62, 57, 52,  0,  0,  0, 33,  0,  2, 12,  6,  0,\n",
       "         45,  0,  0,  0,  0, 43,  0,  0,  0, 25, 61,  0,  0,  0, 62,  0,  0,  0,\n",
       "          0, 13, 13,  0, 57, 45,  0,  0, 33,  4,  0,  0, 30, 63,  2, 11,  0,  0,\n",
       "         30,  0,  0, 48, 47,  0, 43,  0, 44, 32,  0, 27, 57, 27,  0, 13,  0,  0,\n",
       "          0, 59, 41,  0, 38,  0,  0,  7, 29,  0,  0,  0, 34,  0,  0, 55,  4, 48,\n",
       "          0, 46,  0,  0,  0, 59, 22, 43,  0, 10, 22, 35,  0,  0,  0,  0, 25,  0,\n",
       "         25, 42, 32,  0, 42, 12,  0,  0,  0,  0, 32, 32, 45,  0,  0,  0, 32,  0,\n",
       "          0,  0, 22, 53,  0, 19, 19,  0,  0,  0, 30,  2,  0, 49,  0,  2,  0, 15,\n",
       "         48,  0,  0, 21]),\n",
       " tensor([[-809.9097,  949.9618, -809.9097,  949.9618],\n",
       "         [-362.5710,  447.7780, -487.5289,  708.2578],\n",
       "         [-556.5168,  781.8157,  981.4241, -685.8655],\n",
       "         ...,\n",
       "         [ 753.1874,  -23.8420,  753.1874,  -23.8420],\n",
       "         [ 210.1012, -161.8371,  210.1012, -161.8371],\n",
       "         [ -99.7832, -987.2088,  915.9093,  -58.8253]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_data = next(iter(train_loader))\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpAmplitudeModel(nn.Module):\n",
    "    def __init__(self, signal_length=1024, n_bins=64):\n",
    "        super().__init__()\n",
    "        self.n_bins = n_bins\n",
    "        self.bin_size = signal_length // n_bins\n",
    "        \n",
    "        # Feature extractor with dilated convolutions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv1d(2, 16, kernel_size=5, padding=2, dilation=1),\n",
    "            nn.InstanceNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(16, 32, kernel_size=5, padding=4, dilation=2),\n",
    "            nn.InstanceNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=8, dilation=4),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Jump probability head\n",
    "        self.jump_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),  # [batch, 64, 1]\n",
    "            nn.Flatten(),             # [batch, 64]\n",
    "            nn.Linear(64, 1),         # [batch, 1]\n",
    "            nn.Sigmoid()              # [batch, 1]\n",
    "        )\n",
    "        \n",
    "        # Position probability head\n",
    "        self.pos_head = nn.Sequential(\n",
    "            nn.Conv1d(64, n_bins, kernel_size=1),  # [batch, n_bins, signal_length]\n",
    "            nn.AdaptiveAvgPool1d(1),              # [batch, n_bins, 1]\n",
    "            nn.Flatten(start_dim=1),              # [batch, n_bins]\n",
    "            nn.Softmax(dim=1)                     # [batch, n_bins]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        features = self.features(x)  # [batch, 64, signal_length]\n",
    "        \n",
    "        # Jump probability\n",
    "        jump_p = self.jump_head(features)  # [batch, 1]\n",
    "        \n",
    "        # Position probabilities\n",
    "        pos_probs = self.pos_head(features)  # [batch, n_bins]\n",
    "        \n",
    "        return jump_p, pos_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_percentages = {\"jump_loss\": 0.0, \"pos_loss\": 0.0, \"amp_loss\": 0.0, \"entropy_loss\": 0.0}\n",
    "\n",
    "    for X, y_indicator, y_cp, y_amp in dataloader:\n",
    "        X, y_indicator, y_cp, y_amp = X.to(device), y_indicator.to(device), y_cp.to(device), y_amp.to(device)\n",
    "\n",
    "        jump_p, pos_probs = model(X)\n",
    "        pred = (jump_p, pos_probs)\n",
    "\n",
    "        gt = (y_indicator.squeeze(), y_cp.squeeze(), y_amp[:,0], y_amp[:,1], y_amp[:,2], y_amp[:,3])\n",
    "\n",
    "        bin_size = X.size(2) // pos_probs.size(1)  # signal_length // n_bins\n",
    "        loss = criterion(pred, gt, X, bin_size)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, device, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X, y_indicator, y_cp, y_amp in dataloader:\n",
    "        X, y_indicator, y_cp, y_amp = X.to(device), y_indicator.to(device), y_cp.to(device), y_amp.to(device)\n",
    "\n",
    "        jump_p, pos_probs = model(X)\n",
    "        pred = (jump_p, pos_probs)\n",
    "\n",
    "        gt = (y_indicator.squeeze(), y_cp.squeeze(), y_amp[:,0], y_amp[:,1], y_amp[:,2], y_amp[:,3])\n",
    "\n",
    "        bin_size = X.size(2) // pos_probs.size(1)  # signal_length // n_bins\n",
    "        loss = criterion(pred, gt, X, bin_size)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    loss_fn,\n",
    "    num_epochs=20,\n",
    "    plot_loss=True,\n",
    "    scheduler=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model and track performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to train.\n",
    "        train_loader (DataLoader): Training DataLoader.\n",
    "        val_loader (DataLoader): Validation DataLoader.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        device (torch.device): Device to run on (CPU/GPU).\n",
    "        amp_loss_fn (Loss): Loss function for amplitude regression.\n",
    "        cp_loss_fn (Loss): Loss function for change point localization.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        plot_loss (bool): Whether to plot loss curves.\n",
    "\n",
    "    Returns:\n",
    "        trained_model (nn.Module): Best trained model.\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model_wts = model.state_dict()\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, optimizer, device, loss_fn\n",
    "        )\n",
    "        val_loss = validate(model, val_loader, device, loss_fn)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "            print(scheduler.get_last_lr())\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}]  Train Loss: {train_loss:.4f}  Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Training complete. Best Val Loss: {:.4f}\".format(best_val_loss))\n",
    "\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\")\n",
    "        plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amplitudes(x, pos_probs, bin_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: Input signal of shape [batch_size, 2, signal_length]\n",
    "        pos_probs: Position probabilities of shape [batch_size, n_bins]\n",
    "        bin_size: Size of each bin (signal_length // n_bins)\n",
    "    Returns:\n",
    "        start_i: [batch_size] (start in-phase amplitude)\n",
    "        start_q: [batch_size] (start quadrature amplitude)\n",
    "        stop_i: [batch_size] (stop in-phase amplitude)\n",
    "        stop_q: [batch_size] (stop quadrature amplitude)\n",
    "    \"\"\"\n",
    "    batch_size, n_bins = pos_probs.shape\n",
    "    signal_length = x.size(2)\n",
    "    device = x.device\n",
    "\n",
    "    # Compute cumulative sums for efficient mean calculation\n",
    "    cumsum_i = torch.cumsum(x[:, 0, :], dim=1)  # [batch_size, signal_length]\n",
    "    cumsum_q = torch.cumsum(x[:, 1, :], dim=1)  # [batch_size, signal_length]\n",
    "\n",
    "    # Precompute all possible split points\n",
    "    split_points = torch.arange(0, n_bins, device=device) * bin_size  # [n_bins]\n",
    "    split_points = split_points.unsqueeze(0).repeat(batch_size, 1)  # [batch_size, n_bins]\n",
    "\n",
    "    # Compute means for all possible splits\n",
    "    means_i = torch.zeros(batch_size, n_bins, 2, device=device)  # [start, stop]\n",
    "    means_q = torch.zeros_like(means_i)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        t = split_points[:, i]  # [batch_size]\n",
    "        t = torch.clamp(t, 1, signal_length - 1)  # Avoid division by zero\n",
    "\n",
    "        # Start means\n",
    "        means_i[:, i, 0] = cumsum_i.gather(1, t.long().unsqueeze(1)).squeeze() / t\n",
    "        means_q[:, i, 0] = cumsum_q.gather(1, t.long().unsqueeze(1)).squeeze() / t\n",
    "\n",
    "        # Stop means\n",
    "        means_i[:, i, 1] = (cumsum_i[:, -1] - cumsum_i.gather(1, t.long().unsqueeze(1)).squeeze()) / (signal_length - t)\n",
    "        means_q[:, i, 1] = (cumsum_q[:, -1] - cumsum_q.gather(1, t.long().unsqueeze(1)).squeeze()) / (signal_length - t)\n",
    "\n",
    "    # Weight by position probabilities\n",
    "    weighted_start_i = (means_i[..., 0] * pos_probs).sum(dim=1)  # [batch_size]\n",
    "    weighted_stop_i = (means_i[..., 1] * pos_probs).sum(dim=1)  # [batch_size] \n",
    "    weighted_start_q = (means_q[..., 0] * pos_probs).sum(dim=1)  # [batch_size]\n",
    "    weighted_stop_q = (means_q[..., 1] * pos_probs).sum(dim=1)  # [batch_size] \n",
    "\n",
    "    return weighted_start_i, weighted_start_q, weighted_stop_i, weighted_stop_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, pos_weight=1.0, amp_weight=1.0, entropy_weight=0.1):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()  # For jump classification\n",
    "        self.ce = nn.CrossEntropyLoss()  # For position prediction\n",
    "        self.amp_loss = nn.SmoothL1Loss()  # For amplitude regression\n",
    "        self.pos_weight = pos_weight\n",
    "        self.amp_weight = amp_weight\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "    def forward(self, pred, gt, x, bin_size):\n",
    "        jump_p, pos_probs = pred\n",
    "        gt_jump, gt_pos, gt_start_i, gt_start_q, gt_stop_i, gt_stop_q = gt\n",
    "\n",
    "        # Jump classification loss\n",
    "        jump_loss = self.bce(jump_p.squeeze(), gt_jump.float())\n",
    "\n",
    "        # Position loss (only when jump exists)\n",
    "        mask = gt_jump.bool()  # Mask for samples with jumps\n",
    "        if mask.any():\n",
    "            pos_loss = self.ce(pos_probs[mask], gt_pos[mask])\n",
    "        else:\n",
    "            pos_loss = torch.tensor(0.0, device=jump_p.device)\n",
    "\n",
    "        # Amplitude loss\n",
    "        start_i, start_q, stop_i, stop_q = compute_amplitudes(x, pos_probs, bin_size)\n",
    "        amp_loss = self.amp_loss(\n",
    "            torch.vstack([start_i, start_q, stop_i, stop_q]),\n",
    "            torch.vstack([gt_start_i, gt_start_q, gt_stop_i, gt_stop_q])\n",
    "        )\n",
    "\n",
    "        # Entropy regularization (only when jump exists)\n",
    "        entropy = -torch.sum(pos_probs * torch.log(pos_probs + 1e-10), dim=1)  # [batch_size]\n",
    "        entropy_loss = (entropy * gt_jump).mean()  # Apply only to samples with jumps\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = (\n",
    "            jump_loss +\n",
    "            self.pos_weight * pos_loss +\n",
    "            self.amp_weight * amp_loss +\n",
    "            self.entropy_weight * entropy_loss\n",
    "        )\n",
    "\n",
    "        # Compute percentage contributions\n",
    "        loss_components = {\n",
    "            \"jump_loss\": jump_loss.item(),\n",
    "            \"pos_loss\": pos_loss.item() * self.pos_weight,\n",
    "            \"amp_loss\": amp_loss.item() * self.amp_weight,\n",
    "            \"entropy_loss\": entropy_loss.item() * self.entropy_weight,\n",
    "        }\n",
    "        total = sum(loss_components.values())\n",
    "        loss_percentages = {k: (v / total) * 100 for k, v in loss_components.items()}\n",
    "\n",
    "        return total_loss, loss_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 1, 2, 3, 4],\n",
       "         [5, 6, 7, 8, 9]]),\n",
       " tensor([[ 0,  1,  3,  6, 10],\n",
       "         [ 5, 11, 18, 26, 35]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10).reshape(2,5)\n",
    "a, torch.cumsum(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.],\n",
       "        [ 0., 16., 32., 48., 64.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.Tensor([0, 16, 32, 48, 64])\n",
    "split_points = b.unsqueeze(0).repeat(4, 1)  # [4, n_bins]\n",
    "split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = split_points[:, 0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.clamp(t, 1, 80 - 1)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Initialize model and loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = JumpAmplitudeModel().to(device)\n",
    "criterion = CustomLoss(pos_weight=1.0, amp_weight=1.0, entropy_weight=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01]\n",
      "Epoch [1/300]  Train Loss: 119.4858  Val Loss: 119.3340\n",
      "[0.01]\n",
      "Epoch [2/300]  Train Loss: 116.8682  Val Loss: 117.4303\n",
      "[0.01]\n",
      "Epoch [3/300]  Train Loss: 115.3864  Val Loss: 116.2931\n",
      "[0.01]\n",
      "Epoch [4/300]  Train Loss: 114.4012  Val Loss: 115.8731\n",
      "[0.01]\n",
      "Epoch [5/300]  Train Loss: 113.9605  Val Loss: 115.5331\n",
      "[0.01]\n",
      "Epoch [6/300]  Train Loss: 113.0905  Val Loss: 114.4941\n",
      "[0.01]\n",
      "Epoch [7/300]  Train Loss: 112.5401  Val Loss: 114.1092\n",
      "[0.01]\n",
      "Epoch [8/300]  Train Loss: 112.2500  Val Loss: 113.6942\n",
      "[0.01]\n",
      "Epoch [9/300]  Train Loss: 111.9796  Val Loss: 113.1364\n",
      "[0.01]\n",
      "Epoch [10/300]  Train Loss: 111.5899  Val Loss: 113.2030\n",
      "[0.01]\n",
      "Epoch [11/300]  Train Loss: 111.4372  Val Loss: 112.6757\n",
      "[0.01]\n",
      "Epoch [12/300]  Train Loss: 111.0873  Val Loss: 112.7431\n",
      "[0.01]\n",
      "Epoch [13/300]  Train Loss: 110.7079  Val Loss: 111.8972\n",
      "[0.01]\n",
      "Epoch [14/300]  Train Loss: 109.8977  Val Loss: 110.6572\n",
      "[0.01]\n",
      "Epoch [15/300]  Train Loss: 109.0140  Val Loss: 109.4856\n",
      "[0.01]\n",
      "Epoch [16/300]  Train Loss: 107.2299  Val Loss: 107.9999\n",
      "[0.01]\n",
      "Epoch [17/300]  Train Loss: 104.5123  Val Loss: 104.7129\n",
      "[0.01]\n",
      "Epoch [18/300]  Train Loss: 101.4332  Val Loss: 101.8154\n",
      "[0.01]\n",
      "Epoch [19/300]  Train Loss: 99.7866  Val Loss: 100.4025\n",
      "[0.01]\n",
      "Epoch [20/300]  Train Loss: 98.3038  Val Loss: 99.6545\n",
      "[0.01]\n",
      "Epoch [21/300]  Train Loss: 97.1429  Val Loss: 98.0097\n",
      "[0.01]\n",
      "Epoch [22/300]  Train Loss: 96.1244  Val Loss: 97.4655\n",
      "[0.01]\n",
      "Epoch [23/300]  Train Loss: 95.4519  Val Loss: 97.0943\n",
      "[0.01]\n",
      "Epoch [24/300]  Train Loss: 94.9375  Val Loss: 96.9133\n",
      "[0.01]\n",
      "Epoch [25/300]  Train Loss: 94.3074  Val Loss: 95.7915\n",
      "[0.01]\n",
      "Epoch [26/300]  Train Loss: 93.7896  Val Loss: 95.3124\n",
      "[0.01]\n",
      "Epoch [27/300]  Train Loss: 93.2424  Val Loss: 94.5423\n",
      "[0.01]\n",
      "Epoch [28/300]  Train Loss: 92.6587  Val Loss: 93.9505\n",
      "[0.01]\n",
      "Epoch [29/300]  Train Loss: 92.4272  Val Loss: 94.8542\n",
      "[0.01]\n",
      "Epoch [30/300]  Train Loss: 92.1874  Val Loss: 93.6256\n",
      "[0.01]\n",
      "Epoch [31/300]  Train Loss: 91.6348  Val Loss: 93.0877\n",
      "[0.01]\n",
      "Epoch [32/300]  Train Loss: 91.2650  Val Loss: 92.7965\n",
      "[0.01]\n",
      "Epoch [33/300]  Train Loss: 90.7962  Val Loss: 92.6997\n",
      "[0.01]\n",
      "Epoch [34/300]  Train Loss: 90.5029  Val Loss: 92.4057\n",
      "[0.01]\n",
      "Epoch [35/300]  Train Loss: 90.4274  Val Loss: 91.8056\n",
      "[0.01]\n",
      "Epoch [36/300]  Train Loss: 90.0152  Val Loss: 92.4288\n",
      "[0.01]\n",
      "Epoch [37/300]  Train Loss: 89.8996  Val Loss: 92.1218\n",
      "[0.01]\n",
      "Epoch [38/300]  Train Loss: 89.5316  Val Loss: 92.1763\n",
      "[0.01]\n",
      "Epoch [39/300]  Train Loss: 89.3829  Val Loss: 91.1463\n",
      "[0.01]\n",
      "Epoch [40/300]  Train Loss: 89.2363  Val Loss: 90.9676\n",
      "[0.01]\n",
      "Epoch [41/300]  Train Loss: 88.9223  Val Loss: 90.8028\n",
      "[0.01]\n",
      "Epoch [42/300]  Train Loss: 88.8289  Val Loss: 90.7494\n",
      "[0.01]\n",
      "Epoch [43/300]  Train Loss: 88.4897  Val Loss: 90.4553\n",
      "[0.01]\n",
      "Epoch [44/300]  Train Loss: 88.4350  Val Loss: 90.0923\n",
      "[0.01]\n",
      "Epoch [45/300]  Train Loss: 88.2182  Val Loss: 90.5627\n",
      "[0.01]\n",
      "Epoch [46/300]  Train Loss: 87.9977  Val Loss: 90.4985\n",
      "[0.01]\n",
      "Epoch [47/300]  Train Loss: 87.8832  Val Loss: 90.3725\n",
      "[0.01]\n",
      "Epoch [48/300]  Train Loss: 87.7041  Val Loss: 89.8198\n",
      "[0.01]\n",
      "Epoch [49/300]  Train Loss: 87.5185  Val Loss: 90.1299\n",
      "[0.01]\n",
      "Epoch [50/300]  Train Loss: 87.4096  Val Loss: 89.6742\n",
      "[0.01]\n",
      "Epoch [51/300]  Train Loss: 87.2670  Val Loss: 89.7851\n",
      "[0.01]\n",
      "Epoch [52/300]  Train Loss: 87.2713  Val Loss: 89.3520\n",
      "[0.01]\n",
      "Epoch [53/300]  Train Loss: 86.9554  Val Loss: 89.0821\n",
      "[0.01]\n",
      "Epoch [54/300]  Train Loss: 87.0187  Val Loss: 89.2938\n",
      "[0.01]\n",
      "Epoch [55/300]  Train Loss: 86.9268  Val Loss: 88.7952\n",
      "[0.01]\n",
      "Epoch [56/300]  Train Loss: 86.7790  Val Loss: 88.5958\n",
      "[0.01]\n",
      "Epoch [57/300]  Train Loss: 86.5392  Val Loss: 89.1287\n",
      "[0.01]\n",
      "Epoch [58/300]  Train Loss: 86.5738  Val Loss: 89.6681\n",
      "[0.01]\n",
      "Epoch [59/300]  Train Loss: 86.4494  Val Loss: 88.5667\n",
      "[0.01]\n",
      "Epoch [60/300]  Train Loss: 86.3974  Val Loss: 88.2015\n",
      "[0.01]\n",
      "Epoch [61/300]  Train Loss: 86.2606  Val Loss: 88.6845\n",
      "[0.01]\n",
      "Epoch [62/300]  Train Loss: 86.1653  Val Loss: 87.9702\n",
      "[0.01]\n",
      "Epoch [63/300]  Train Loss: 85.9904  Val Loss: 88.0387\n",
      "[0.01]\n",
      "Epoch [64/300]  Train Loss: 86.0638  Val Loss: 88.2359\n",
      "[0.01]\n",
      "Epoch [65/300]  Train Loss: 85.9566  Val Loss: 87.9131\n",
      "[0.01]\n",
      "Epoch [66/300]  Train Loss: 85.7434  Val Loss: 88.3395\n",
      "[0.01]\n",
      "Epoch [67/300]  Train Loss: 85.7658  Val Loss: 88.2375\n",
      "[0.01]\n",
      "Epoch [68/300]  Train Loss: 85.6166  Val Loss: 88.2556\n",
      "[0.01]\n",
      "Epoch [69/300]  Train Loss: 85.7697  Val Loss: 87.6515\n",
      "[0.01]\n",
      "Epoch [70/300]  Train Loss: 85.5904  Val Loss: 87.6773\n",
      "[0.01]\n",
      "Epoch [71/300]  Train Loss: 85.5551  Val Loss: 87.7993\n",
      "[0.01]\n",
      "Epoch [72/300]  Train Loss: 85.4196  Val Loss: 87.8039\n",
      "[0.01]\n",
      "Epoch [73/300]  Train Loss: 85.4854  Val Loss: 87.4820\n",
      "[0.01]\n",
      "Epoch [74/300]  Train Loss: 85.3552  Val Loss: 87.5099\n",
      "[0.01]\n",
      "Epoch [75/300]  Train Loss: 85.1909  Val Loss: 87.5255\n",
      "[0.01]\n",
      "Epoch [76/300]  Train Loss: 85.2694  Val Loss: 87.6441\n",
      "[0.01]\n",
      "Epoch [77/300]  Train Loss: 85.1588  Val Loss: 87.4431\n",
      "[0.01]\n",
      "Epoch [78/300]  Train Loss: 85.0984  Val Loss: 87.4963\n",
      "[0.01]\n",
      "Epoch [79/300]  Train Loss: 85.1314  Val Loss: 87.4795\n",
      "[0.01]\n",
      "Epoch [80/300]  Train Loss: 84.8753  Val Loss: 87.5194\n",
      "[0.01]\n",
      "Epoch [81/300]  Train Loss: 84.9755  Val Loss: 88.7416\n",
      "[0.01]\n",
      "Epoch [82/300]  Train Loss: 84.9321  Val Loss: 86.9204\n",
      "[0.01]\n",
      "Epoch [83/300]  Train Loss: 84.7336  Val Loss: 87.1266\n",
      "[0.01]\n",
      "Epoch [84/300]  Train Loss: 84.6613  Val Loss: 87.3269\n",
      "[0.01]\n",
      "Epoch [85/300]  Train Loss: 84.6291  Val Loss: 87.2859\n",
      "[0.01]\n",
      "Epoch [86/300]  Train Loss: 84.7316  Val Loss: 87.1795\n",
      "[0.01]\n",
      "Epoch [87/300]  Train Loss: 84.6865  Val Loss: 87.6751\n",
      "[0.001]\n",
      "Epoch [88/300]  Train Loss: 84.6626  Val Loss: 87.6682\n",
      "[0.001]\n",
      "Epoch [89/300]  Train Loss: 83.8472  Val Loss: 86.5127\n",
      "[0.001]\n",
      "Epoch [90/300]  Train Loss: 83.6466  Val Loss: 86.5227\n",
      "[0.001]\n",
      "Epoch [91/300]  Train Loss: 83.6263  Val Loss: 86.4776\n",
      "[0.001]\n",
      "Epoch [92/300]  Train Loss: 83.6697  Val Loss: 86.4142\n",
      "[0.001]\n",
      "Epoch [93/300]  Train Loss: 83.6147  Val Loss: 86.4257\n",
      "[0.001]\n",
      "Epoch [94/300]  Train Loss: 83.5576  Val Loss: 86.4870\n",
      "[0.001]\n",
      "Epoch [95/300]  Train Loss: 83.6173  Val Loss: 86.4561\n",
      "[0.001]\n",
      "Epoch [96/300]  Train Loss: 83.5581  Val Loss: 86.4352\n",
      "[0.001]\n",
      "Epoch [97/300]  Train Loss: 83.5279  Val Loss: 86.4702\n",
      "[0.0001]\n",
      "Epoch [98/300]  Train Loss: 83.5686  Val Loss: 86.4856\n",
      "[0.0001]\n",
      "Epoch [99/300]  Train Loss: 83.4804  Val Loss: 86.4291\n",
      "[0.0001]\n",
      "Epoch [100/300]  Train Loss: 83.3498  Val Loss: 86.4202\n",
      "[0.0001]\n",
      "Epoch [101/300]  Train Loss: 83.3847  Val Loss: 86.4251\n",
      "[0.0001]\n",
      "Epoch [102/300]  Train Loss: 83.4280  Val Loss: 86.4223\n",
      "[0.0001]\n",
      "Epoch [103/300]  Train Loss: 83.4160  Val Loss: 86.4133\n",
      "[1e-05]\n",
      "Epoch [104/300]  Train Loss: 83.3910  Val Loss: 86.4228\n",
      "[1e-05]\n",
      "Epoch [105/300]  Train Loss: 83.4085  Val Loss: 86.4219\n",
      "[1e-05]\n",
      "Epoch [106/300]  Train Loss: 83.3837  Val Loss: 86.4210\n",
      "[1e-05]\n",
      "Epoch [107/300]  Train Loss: 83.3826  Val Loss: 86.4204\n",
      "[1e-05]\n",
      "Epoch [108/300]  Train Loss: 83.4415  Val Loss: 86.4207\n",
      "[1e-05]\n",
      "Epoch [109/300]  Train Loss: 83.3795  Val Loss: 86.4205\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [110/300]  Train Loss: 83.3366  Val Loss: 86.4197\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [111/300]  Train Loss: 83.4110  Val Loss: 86.4195\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [112/300]  Train Loss: 83.4272  Val Loss: 86.4195\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [113/300]  Train Loss: 83.4375  Val Loss: 86.4194\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [114/300]  Train Loss: 83.3988  Val Loss: 86.4195\n",
      "[1.0000000000000002e-06]\n",
      "Epoch [115/300]  Train Loss: 83.4216  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [116/300]  Train Loss: 83.4123  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [117/300]  Train Loss: 83.4053  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [118/300]  Train Loss: 83.3509  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [119/300]  Train Loss: 83.3780  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [120/300]  Train Loss: 83.3483  Val Loss: 86.4196\n",
      "[1.0000000000000002e-07]\n",
      "Epoch [121/300]  Train Loss: 83.4205  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [122/300]  Train Loss: 83.4588  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [123/300]  Train Loss: 83.4117  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [124/300]  Train Loss: 83.4366  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [125/300]  Train Loss: 83.3760  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [126/300]  Train Loss: 83.4555  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [127/300]  Train Loss: 83.3695  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [128/300]  Train Loss: 83.4598  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [129/300]  Train Loss: 83.3614  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [130/300]  Train Loss: 83.3655  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [131/300]  Train Loss: 83.3996  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [132/300]  Train Loss: 83.4629  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [133/300]  Train Loss: 83.3963  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [134/300]  Train Loss: 83.3620  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [135/300]  Train Loss: 83.4084  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [136/300]  Train Loss: 83.3970  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [137/300]  Train Loss: 83.4244  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [138/300]  Train Loss: 83.4263  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [139/300]  Train Loss: 83.3930  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [140/300]  Train Loss: 83.4608  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [141/300]  Train Loss: 83.3339  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [142/300]  Train Loss: 83.3897  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [143/300]  Train Loss: 83.4343  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [144/300]  Train Loss: 83.5260  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [145/300]  Train Loss: 83.3474  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [146/300]  Train Loss: 83.3388  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [147/300]  Train Loss: 83.4285  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [148/300]  Train Loss: 83.3738  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [149/300]  Train Loss: 83.3848  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [150/300]  Train Loss: 83.3911  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [151/300]  Train Loss: 83.4639  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [152/300]  Train Loss: 83.4102  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [153/300]  Train Loss: 83.4679  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [154/300]  Train Loss: 83.4033  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [155/300]  Train Loss: 83.3958  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [156/300]  Train Loss: 83.3665  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [157/300]  Train Loss: 83.3757  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [158/300]  Train Loss: 83.3810  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [159/300]  Train Loss: 83.3504  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [160/300]  Train Loss: 83.3799  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [161/300]  Train Loss: 83.4064  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [162/300]  Train Loss: 83.4119  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [163/300]  Train Loss: 83.4295  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [164/300]  Train Loss: 83.4268  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [165/300]  Train Loss: 83.3525  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [166/300]  Train Loss: 83.3877  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [167/300]  Train Loss: 83.3612  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [168/300]  Train Loss: 83.3832  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [169/300]  Train Loss: 83.4376  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [170/300]  Train Loss: 83.3543  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [171/300]  Train Loss: 83.4085  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [172/300]  Train Loss: 83.4033  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [173/300]  Train Loss: 83.3394  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [174/300]  Train Loss: 83.4007  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [175/300]  Train Loss: 83.3506  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [176/300]  Train Loss: 83.4098  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [177/300]  Train Loss: 83.3771  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [178/300]  Train Loss: 83.3938  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [179/300]  Train Loss: 83.4450  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [180/300]  Train Loss: 83.4366  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [181/300]  Train Loss: 83.4051  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [182/300]  Train Loss: 83.4316  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [183/300]  Train Loss: 83.3908  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [184/300]  Train Loss: 83.4262  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [185/300]  Train Loss: 83.3495  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [186/300]  Train Loss: 83.4244  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [187/300]  Train Loss: 83.3874  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [188/300]  Train Loss: 83.3736  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [189/300]  Train Loss: 83.4146  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [190/300]  Train Loss: 83.3814  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [191/300]  Train Loss: 83.4170  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [192/300]  Train Loss: 83.4311  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [193/300]  Train Loss: 83.3818  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [194/300]  Train Loss: 83.3728  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [195/300]  Train Loss: 83.4214  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [196/300]  Train Loss: 83.3610  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [197/300]  Train Loss: 83.4001  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [198/300]  Train Loss: 83.4201  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [199/300]  Train Loss: 83.4640  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [200/300]  Train Loss: 83.3438  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [201/300]  Train Loss: 83.3722  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [202/300]  Train Loss: 83.3169  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [203/300]  Train Loss: 83.4447  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [204/300]  Train Loss: 83.3730  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [205/300]  Train Loss: 83.3870  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [206/300]  Train Loss: 83.4405  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [207/300]  Train Loss: 83.3819  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [208/300]  Train Loss: 83.3625  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [209/300]  Train Loss: 83.3787  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [210/300]  Train Loss: 83.3535  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [211/300]  Train Loss: 83.4108  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [212/300]  Train Loss: 83.4080  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [213/300]  Train Loss: 83.3430  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [214/300]  Train Loss: 83.4805  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [215/300]  Train Loss: 83.4199  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [216/300]  Train Loss: 83.4204  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [217/300]  Train Loss: 83.3849  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [218/300]  Train Loss: 83.4199  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [219/300]  Train Loss: 83.3451  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [220/300]  Train Loss: 83.4499  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [221/300]  Train Loss: 83.4567  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [222/300]  Train Loss: 83.3835  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [223/300]  Train Loss: 83.4147  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [224/300]  Train Loss: 83.3660  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [225/300]  Train Loss: 83.3784  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [226/300]  Train Loss: 83.3587  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [227/300]  Train Loss: 83.4499  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [228/300]  Train Loss: 83.3457  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [229/300]  Train Loss: 83.3364  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [230/300]  Train Loss: 83.4053  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [231/300]  Train Loss: 83.3950  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [232/300]  Train Loss: 83.4096  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [233/300]  Train Loss: 83.4080  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [234/300]  Train Loss: 83.3427  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [235/300]  Train Loss: 83.4112  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [236/300]  Train Loss: 83.3881  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [237/300]  Train Loss: 83.4550  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [238/300]  Train Loss: 83.3922  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [239/300]  Train Loss: 83.3838  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [240/300]  Train Loss: 83.3952  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [241/300]  Train Loss: 83.4172  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [242/300]  Train Loss: 83.3525  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [243/300]  Train Loss: 83.3745  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [244/300]  Train Loss: 83.3712  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [245/300]  Train Loss: 83.3549  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [246/300]  Train Loss: 83.3961  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [247/300]  Train Loss: 83.4166  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [248/300]  Train Loss: 83.3511  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [249/300]  Train Loss: 83.4689  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [250/300]  Train Loss: 83.3879  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [251/300]  Train Loss: 83.3739  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [252/300]  Train Loss: 83.3819  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [253/300]  Train Loss: 83.4373  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [254/300]  Train Loss: 83.3388  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [255/300]  Train Loss: 83.4489  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [256/300]  Train Loss: 83.3973  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [257/300]  Train Loss: 83.3270  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [258/300]  Train Loss: 83.3872  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [259/300]  Train Loss: 83.3727  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [260/300]  Train Loss: 83.3981  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [261/300]  Train Loss: 83.3750  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [262/300]  Train Loss: 83.4147  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [263/300]  Train Loss: 83.3926  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [264/300]  Train Loss: 83.4090  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [265/300]  Train Loss: 83.3804  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [266/300]  Train Loss: 83.4162  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [267/300]  Train Loss: 83.3356  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [268/300]  Train Loss: 83.3915  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [269/300]  Train Loss: 83.3351  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [270/300]  Train Loss: 83.3402  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [271/300]  Train Loss: 83.3678  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [272/300]  Train Loss: 83.3363  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [273/300]  Train Loss: 83.4325  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [274/300]  Train Loss: 83.3277  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [275/300]  Train Loss: 83.4038  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [276/300]  Train Loss: 83.4248  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [277/300]  Train Loss: 83.3896  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [278/300]  Train Loss: 83.3677  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [279/300]  Train Loss: 83.3644  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [280/300]  Train Loss: 83.3735  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [281/300]  Train Loss: 83.4529  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [282/300]  Train Loss: 83.4072  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [283/300]  Train Loss: 83.3470  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [284/300]  Train Loss: 83.4194  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [285/300]  Train Loss: 83.3886  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [286/300]  Train Loss: 83.4845  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [287/300]  Train Loss: 83.3929  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [288/300]  Train Loss: 83.3594  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [289/300]  Train Loss: 83.3723  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [290/300]  Train Loss: 83.3430  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [291/300]  Train Loss: 83.3858  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [292/300]  Train Loss: 83.3930  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [293/300]  Train Loss: 83.4033  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [294/300]  Train Loss: 83.4142  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [295/300]  Train Loss: 83.3535  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [296/300]  Train Loss: 83.3765  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [297/300]  Train Loss: 83.4164  Val Loss: 86.4196\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [298/300]  Train Loss: 83.3840  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [299/300]  Train Loss: 83.4396  Val Loss: 86.4197\n",
      "[1.0000000000000004e-08]\n",
      "Epoch [300/300]  Train Loss: 83.4516  Val Loss: 86.4197\n",
      "Training complete. Best Val Loss: 86.4133\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model1 = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        optimizer,\n",
    "        device,\n",
    "        criterion,\n",
    "        num_epochs=300,\n",
    "        scheduler=scheduler\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 2102.2769,   407.7083,   170.0623,  ...,  -898.3487,\n",
       "           -2779.2090,  -816.0172],\n",
       "          [-2056.9741,  2940.7517,  1271.7192,  ..., -1243.4318,\n",
       "           -2281.5813, -2420.2747]],\n",
       " \n",
       "         [[  -12.9912,  -236.6904,   -77.3014,  ..., -1330.9393,\n",
       "            -317.2576,   305.1830],\n",
       "          [  567.3638, -3496.4246,   -89.3775,  ...,  -790.9786,\n",
       "           -2254.7686, -1884.8403]],\n",
       " \n",
       "         [[ 5115.2349,   499.6626,   948.3342,  ...,  -529.3042,\n",
       "             712.9390,  1056.0459],\n",
       "          [  732.2154,  1106.1027,  3165.0308,  ...,   296.9810,\n",
       "             508.7326,   -70.4058]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[  172.2278,   223.4360, -1102.0370,  ...,   328.7923,\n",
       "            -100.4184, -2901.3333],\n",
       "          [  429.5121,  2465.8386, -1451.0809,  ...,   360.3534,\n",
       "            3942.1348,   131.3292]],\n",
       " \n",
       "         [[  -60.4660, -1853.9871, -2994.9580,  ...,  2161.3457,\n",
       "            2394.0864, -2965.7480],\n",
       "          [-1313.1956,  -321.1489, -4373.6396,  ...,     7.8354,\n",
       "            1946.0480,  2086.9888]],\n",
       " \n",
       "         [[ 2769.6006,   383.2892,  1463.9867,  ...,   686.3732,\n",
       "             898.8889,  1053.2865],\n",
       "          [ 1754.6786,   560.2713,  1306.9143,  ..., -3923.4768,\n",
       "           -4232.8140,   836.0437]]]),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]]),\n",
       " tensor([19, 56,  0,  0,  0,  0,  0,  0,  0,  0, 29,  0,  0, 22,  0, 61,  0,  0,\n",
       "          0, 53,  4, 13,  0,  0, 35, 11,  0,  0, 33, 24, 24,  0, 12, 49,  0, 42,\n",
       "         23,  0,  3, 29,  0, 57, 18, 31,  0,  0, 57,  0,  0, 14,  0, 15, 58, 20,\n",
       "         40,  0,  0,  3, 17,  0,  0,  0, 52,  0, 34,  0,  0,  0,  0,  5, 39,  0,\n",
       "         15,  0, 63,  0,  0,  0, 56, 40,  0, 40,  0,  0, 26, 34, 37,  0, 58, 32,\n",
       "          0, 23,  0,  0, 58,  0,  0,  1,  0,  0,  0, 14, 36,  0,  0,  0,  5,  0,\n",
       "         48, 50, 24, 22,  0,  0, 23,  0,  0,  0,  0,  0, 63,  0,  0,  2, 44,  0,\n",
       "         18,  0,  0,  0,  0, 31,  0, 46, 59,  0,  6, 63, 47,  0,  0,  9, 32,  0,\n",
       "          0,  0, 58,  0, 28,  0,  0, 11,  0, 22, 25,  0,  0, 33, 16,  0,  6, 24,\n",
       "          0,  0,  0, 11,  9,  0, 12, 55,  0, 56, 43,  0,  0, 10, 32,  0,  4,  0,\n",
       "         49, 63, 28, 54, 51, 45,  0,  0,  4, 30,  0,  0, 36,  0, 53,  0,  0,  0,\n",
       "          0, 48,  0, 32,  7,  5,  0,  1,  0, 36,  0,  0,  0,  0,  0,  0,  0, 24,\n",
       "          0,  0,  0, 41, 39,  0,  0,  0, 57,  0,  0, 43,  0,  0,  0, 15,  6,  0,\n",
       "          7, 45, 58, 14,  0,  0, 28,  0,  0, 30,  0, 38,  1,  0,  0, 25,  0,  9,\n",
       "          0,  0, 49, 58]),\n",
       " tensor([[ 153.7832,  163.4942, -922.1494, -952.9467],\n",
       "         [-399.5777, -869.5340,  394.2289,  136.9625],\n",
       "         [ 445.8357,  -15.3292,  445.8357,  -15.3292],\n",
       "         ...,\n",
       "         [-454.0664,  364.9399, -481.5957,  331.9081],\n",
       "         [ -56.7942, -234.2085, -579.2318, -156.2706],\n",
       "         [  17.9769,  196.2753,  354.4303, -854.1609]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dummy = next(iter(val_loader))\n",
    "test_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 153.7832,  163.4942, -922.1494, -952.9467],\n",
       "        [-399.5777, -869.5340,  394.2289,  136.9625],\n",
       "        [ 445.8357,  -15.3292,  445.8357,  -15.3292],\n",
       "        ...,\n",
       "        [-454.0664,  364.9399, -481.5957,  331.9081],\n",
       "        [ -56.7942, -234.2085, -579.2318, -156.2706],\n",
       "        [  17.9769,  196.2753,  354.4303, -854.1609]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dummy[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trained_model1(test_dummy[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.0625, grad_fn=<BinaryCrossEntropyBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.BCELoss()\n",
    "loss(test_dummy[1], torch.round(pred[0]).to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8594)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics.functional import accuracy\n",
    "accuracy(test_dummy[1], torch.round(pred[0]).to(\"cpu\"), task=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9736e+02,  0.0000e+00,  0.0000e+00,  6.0725e+02,  8.3568e+02,\n",
       "         5.8925e+02,  1.9452e+02, -8.4051e+02, -7.3593e+02, -3.0441e+02,\n",
       "        -5.6188e+02,  4.9607e+02, -5.4266e+02, -7.6442e+01, -4.9627e+02,\n",
       "        -8.6523e+02, -6.1714e+01, -9.7069e+02, -2.0469e+01,  5.9362e+02,\n",
       "         0.0000e+00,  6.1139e+02,  1.0162e+03,  0.0000e+00,  3.5237e+01,\n",
       "        -5.2420e+02, -3.5259e+02,  0.0000e+00,  1.0360e+03,  1.8568e+02,\n",
       "        -1.4446e+02,  4.5778e+02, -7.7671e+02,  7.8626e+02,  4.4416e+02,\n",
       "         0.0000e+00,  0.0000e+00, -3.0204e+02,  0.0000e+00,  6.1407e+02,\n",
       "        -8.4785e+02,  5.8763e+02, -6.6218e+02, -5.8633e+02,  7.3092e+02,\n",
       "         7.2447e+02,  2.2453e+02,  5.3550e+02, -7.0271e+02,  2.3558e+02,\n",
       "        -9.7118e+02,  0.0000e+00,  0.0000e+00, -7.1487e+02,  3.0978e+02,\n",
       "         3.3180e+02, -1.6380e+02,  1.2399e+02,  0.0000e+00,  3.9770e+02,\n",
       "        -7.7099e+02, -6.6387e+02,  3.6458e+02, -6.7718e+01, -5.2847e+02,\n",
       "        -6.1017e+01,  2.3295e+02,  0.0000e+00, -4.9905e+02, -3.5434e+02,\n",
       "         0.0000e+00,  0.0000e+00, -3.5885e+02,  5.9121e+02,  0.0000e+00,\n",
       "        -4.8636e+02,  3.7884e-01,  6.2353e+02,  0.0000e+00,  7.1324e+02,\n",
       "         9.0348e+02,  0.0000e+00,  0.0000e+00, -7.4495e+02, -2.1375e+02,\n",
       "        -2.2578e+02, -1.4834e+02, -8.0980e+02,  1.2089e+02,  0.0000e+00,\n",
       "        -4.6853e+02, -1.7243e+02,  1.4450e+02,  9.2092e+02,  0.0000e+00,\n",
       "         4.0991e+02,  0.0000e+00,  8.7902e+02, -7.3379e+01,  4.9072e+02,\n",
       "         8.7573e+02,  4.8669e+02,  2.4144e+02,  6.1404e+02, -2.2750e+02,\n",
       "         0.0000e+00,  4.7407e+02, -4.4379e+02,  6.2221e+02,  0.0000e+00,\n",
       "        -1.4680e+02,  3.3632e+02,  1.0856e+02,  2.4721e+02,  1.9545e+01,\n",
       "         3.3788e+02, -1.3004e+02,  2.9555e+02,  6.1286e+02, -7.2956e+02,\n",
       "         9.5771e+02,  9.9858e+02,  9.1323e+02,  0.0000e+00, -3.3728e+02,\n",
       "         3.1463e+02, -8.5670e+02,  0.0000e+00,  0.0000e+00,  6.6061e+02,\n",
       "        -4.8077e+02,  1.6077e+02,  5.5344e+02,  2.0676e+02,  0.0000e+00,\n",
       "        -3.9850e+02, -7.6507e+02, -5.3903e+02,  0.0000e+00,  1.0459e+03,\n",
       "        -3.5812e+02,  3.2920e+02, -9.0964e+02,  0.0000e+00, -1.0090e+03,\n",
       "        -3.2630e+02,  0.0000e+00, -8.2431e+02, -8.0718e+02,  1.2741e+01,\n",
       "         0.0000e+00,  0.0000e+00, -6.8389e+02,  0.0000e+00,  5.7075e+02,\n",
       "        -9.2080e+02,  0.0000e+00, -6.3532e+02,  3.5553e+02,  0.0000e+00,\n",
       "        -1.4493e+02,  2.5377e+02, -5.3136e+02, -8.3029e+02,  0.0000e+00,\n",
       "         8.0691e+02,  0.0000e+00,  6.3382e+02,  0.0000e+00,  0.0000e+00,\n",
       "        -5.9974e+02,  0.0000e+00,  9.0268e+02, -5.2170e+02,  2.6483e+02,\n",
       "         0.0000e+00,  1.0202e+03,  2.0200e+02,  0.0000e+00, -7.4939e+02,\n",
       "         2.1836e+01, -7.1019e+02,  1.8869e+02,  8.2147e+02,  5.0899e+02,\n",
       "         0.0000e+00, -6.2842e+02,  0.0000e+00,  0.0000e+00,  5.7362e+02,\n",
       "        -9.2705e+02, -1.0286e+02,  0.0000e+00,  0.0000e+00, -4.8568e+02,\n",
       "         2.3867e+02,  8.4393e+02,  5.3679e+02, -1.9655e+02,  0.0000e+00,\n",
       "         3.0862e+02, -4.1853e+02, -6.8654e+02, -8.4202e+02,  7.5353e+02,\n",
       "         8.3430e+02,  0.0000e+00, -4.2986e+01,  8.8490e+02,  2.8052e+02,\n",
       "        -3.9544e+02, -1.1324e+02,  6.1102e+02, -1.2806e+02,  0.0000e+00,\n",
       "        -6.5429e+02,  6.4760e+02, -3.3525e+02, -5.9099e+02, -2.3975e+02,\n",
       "         0.0000e+00, -9.6412e+02,  0.0000e+00, -3.6480e+02,  0.0000e+00,\n",
       "         7.6186e+02,  0.0000e+00,  1.6886e+02,  1.0376e+03, -6.4074e+02,\n",
       "         1.8513e+02, -7.0876e+02, -4.1430e+02,  4.3238e+02,  6.4851e+01,\n",
       "         9.6674e+02,  0.0000e+00,  7.9963e+02, -7.5091e+02,  8.3661e+02,\n",
       "         2.4387e+02,  0.0000e+00,  3.9109e+02, -8.2756e+02,  4.0512e+01,\n",
       "        -1.0766e+02,  1.5139e+02, -8.7420e+02,  5.1480e+01,  3.1895e+00,\n",
       "         7.4406e+02, -1.8898e+01,  0.0000e+00, -4.9883e+02,  0.0000e+00,\n",
       "         0.0000e+00], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_amplitudes(test_dummy[0], torch.round(pred[1]).to(\"cpu\"), 16)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
